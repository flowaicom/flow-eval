{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quickstart\n",
        "\n",
        "This tutorial demonstrates how to use the `flow-eval` library to perform language model-based evaluations using `Flow-Judge-v0.1` or any other model supported by the library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running an evaluation\n",
        "\n",
        "Running an evaluation is as simple as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:datasets:PyTorch version 2.4.0 available.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 01-23 19:28:27 awq_marlin.py:90] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
            "WARNING 01-23 19:28:27 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "INFO 01-23 19:28:27 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='flowaicom/Flow-Judge-v0.1-AWQ', speculative_config=None, tokenizer='flowaicom/Flow-Judge-v0.1-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=flowaicom/Flow-Judge-v0.1-AWQ, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "INFO 01-23 19:28:28 model_runner.py:1014] Starting to load model flowaicom/Flow-Judge-v0.1-AWQ...\n",
            "INFO 01-23 19:28:28 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
            "INFO 01-23 19:28:29 weight_utils.py:287] No model.safetensors.index.json found in remote.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f04bd794ec5414f9f60fba85ad041bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 01-23 19:28:30 model_runner.py:1025] Loading model weights took 2.1717 GB\n",
            "INFO 01-23 19:28:31 gpu_executor.py:122] # GPU blocks: 3084, # CPU blocks: 682\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.14s/it, est. speed input: 299.97 toks/s, output: 44.11 toks/s]\n"
          ]
        }
      ],
      "source": [
        "from flow_eval.lm.models import Vllm, Llamafile, Hf, Baseten\n",
        "from flow_eval.lm.metrics import RESPONSE_FAITHFULNESS_5POINT\n",
        "from flow_eval.core import EvalInput\n",
        "from flow_eval import LMEvaluator\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "# If you are running on an Ampere GPU or newer, create a model using VLLM\n",
        "model = Vllm()\n",
        "\n",
        "# If you have other applications open taking up VRAM, you can use less VRAM by setting gpu_memory_utilization to a lower value.\n",
        "# model = Vllm(gpu_memory_utilization=0.70)\n",
        "\n",
        "# Or if not running on Ampere GPU or newer, create a model using no flash attn and Hugging Face Transformers\n",
        "# model = Hf(flash_attn=False)\n",
        "\n",
        "# Or create a model using Baseten if you don't want to run locally.\n",
        "# As a pre-requisite step:\n",
        "#  - Sign up to Baseten\n",
        "#  - Generate an api key https://app.baseten.co/settings/api_keys\n",
        "#  - Set the api key as an environment variable & initialize:\n",
        "# import os\n",
        "# os.environ[\"BASETEN_API_KEY\"] = \"your_api_key\"\n",
        "# model = Baseten()\n",
        "\n",
        "# Or create a model using Llamafile if not running an Nvidia GPU & running a Silicon MacOS for example\n",
        "# model = Llamafile()\n",
        "\n",
        "# Initialize the judge\n",
        "faithfulness_evaluator = LMEvaluator(\n",
        "    eval=RESPONSE_FAITHFULNESS_5POINT,\n",
        "    model=model\n",
        ")\n",
        "\n",
        "# Sample to evaluate\n",
        "query = \"\"\"Please read the technical issue that the user is facing and help me create a detailed solution based on the context provided.\"\"\"\n",
        "context = \"\"\"# Customer Issue:\n",
        "I'm having trouble when uploading a git lfs tracked file to my repo: (base)  bernardo@bernardo-desktop  ~/repos/lm-evaluation-harness  ↱ Flow-Judge-v0.1_evals  git push\n",
        "batch response: This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.\n",
        "\n",
        "# Documentation:\n",
        "Configuring Git Large File Storage\n",
        "Once Git LFS is installed, you need to associate it with a large file in your repository.\n",
        "\n",
        "Platform navigation\n",
        "Mac\n",
        "Windows\n",
        "Linux\n",
        "If there are existing files in your repository that you'd like to use GitHub with, you need to first remove them from the repository and then add them to Git LFS locally. For more information, see \"Moving a file in your repository to Git Large File Storage.\"\n",
        "\n",
        "If there are referenced Git LFS files that did not upload successfully, you will receive an error message. For more information, see \"Resolving Git Large File Storage upload failures.\"\n",
        "\n",
        "Open Terminal.\n",
        "\n",
        "Change your current working directory to an existing repository you'd like to use with Git LFS.\n",
        "\n",
        "To associate a file type in your repository with Git LFS, enter git lfs track followed by the name of the file extension you want to automatically upload to Git LFS.\n",
        "\n",
        "For example, to associate a .psd file, enter the following command:\n",
        "\n",
        "$ git lfs track \"*.psd\"\n",
        "> Tracking \"*.psd\"\n",
        "Every file type you want to associate with Git LFS will need to be added with git lfs track. This command amends your repository's .gitattributes file and associates large files with Git LFS.\n",
        "\n",
        "Note: We strongly suggest that you commit your local .gitattributes file into your repository.\n",
        "\n",
        "Relying on a global .gitattributes file associated with Git LFS may cause conflicts when contributing to other Git projects.\n",
        "Including the .gitattributes file in the repository allows people creating forks or fresh clones to more easily collaborate using Git LFS.\n",
        "Including the .gitattributes file in the repository allows Git LFS objects to optionally be included in ZIP file and tarball archives.\n",
        "Add a file to the repository matching the extension you've associated:\n",
        "\n",
        "git add path/to/file.psd\n",
        "Commit the file and push it to GitHub:\n",
        "\n",
        "git commit -m \"add file.psd\"\n",
        "git push\n",
        "You should see some diagnostic information about your file upload:\n",
        "\n",
        "> Sending file.psd\n",
        "> 44.74 MB / 81.04 MB  55.21 % 14s\n",
        "> 64.74 MB / 81.04 MB  79.21 % 3s\"\"\"\n",
        "response = \"\"\"# Resolving Git LFS Data Quota Issue\n",
        "\n",
        "## Problem\n",
        "The user is encountering an error when trying to push Git LFS tracked files:\n",
        "```\n",
        "This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.\n",
        "```\n",
        "\n",
        "## Solution\n",
        "\n",
        "1. **Understand the issue**\n",
        "   - The error message indicates that the repository has exceeded its data quota for Git LFS (Large File Storage).\n",
        "   - This is likely due to the size or number of large files being tracked by Git LFS.\n",
        "\n",
        "2. **Check current LFS usage**\n",
        "   - Run the following command to see the current LFS objects:\n",
        "     ```\n",
        "     git lfs ls-files\n",
        "     ```\n",
        "   - This will show you which files are being tracked by Git LFS and their sizes.\n",
        "\n",
        "3. **Review Git LFS pricing and quotas**\n",
        "   - Visit GitHub's pricing page to understand the current LFS storage and bandwidth limits for your account type.\n",
        "   - Free accounts typically have lower limits compared to paid plans.\n",
        "\n",
        "4. **Options to resolve the issue**\n",
        "\n",
        "   a. **Purchase more data packs**\n",
        "      - If you need to keep all current LFS objects and continue using LFS extensively:\n",
        "        1. Go to your GitHub account settings.\n",
        "        2. Navigate to the Billing section.\n",
        "        3. Look for options to purchase additional data packs for Git LFS.\n",
        "\n",
        "   b. **Optimize repository**\n",
        "      - If purchasing more data is not an option, consider optimizing your repository:\n",
        "        1. Remove unnecessary large files from LFS tracking.\n",
        "        2. Use `.gitignore` to exclude large files that don't need version control.\n",
        "        3. Consider using alternative storage solutions for very large assets.\n",
        "\n",
        "   c. **Clean up LFS cache**\n",
        "      - Sometimes, cleaning up the LFS cache can help:\n",
        "        ```\n",
        "        git lfs prune\n",
        "        ```\n",
        "\n",
        "5. **Reconfigure Git LFS tracking**\n",
        "   - Review your `.gitattributes` file to ensure only necessary file types are tracked:\n",
        "     ```\n",
        "     git lfs track\n",
        "     ```\n",
        "   - Modify tracking as needed:\n",
        "     ```\n",
        "     git lfs track \"*.psd\"  # Track PSD files\n",
        "     git lfs untrack \"*.zip\"  # Stop tracking ZIP files\n",
        "     ```\n",
        "\n",
        "6. **Commit changes and retry push**\n",
        "   - After making necessary changes:\n",
        "     ```\n",
        "     git add .gitattributes\n",
        "     git commit -m \"Update Git LFS tracking\"\n",
        "     git push\n",
        "     ```\n",
        "\n",
        "7. **If issues persist**\n",
        "   - Contact GitHub support for further assistance.\n",
        "   - They may be able to provide more specific guidance based on your account and repository details.\n",
        "\n",
        "Remember to regularly monitor your Git LFS usage to avoid hitting quotas in the future. Consider setting up alerts or regularly checking your GitHub account's storage usage statistics.\"\"\"\n",
        "\n",
        "# Create an EvalInput\n",
        "# We want to evaluate the response to the customer issue based on the context and the user instructions\n",
        "eval_input = EvalInput(\n",
        "    inputs=[\n",
        "        {\"query\": query},\n",
        "        {\"context\": context},\n",
        "    ],\n",
        "    output={\"response\": response},\n",
        ")\n",
        "\n",
        "# Run the evaluation\n",
        "result = faithfulness_evaluator.evaluate(eval_input, save_results=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "__Feedback:__\n",
              "The response provided is mostly consistent with the context given. It addresses the main issue of the user encountering a data quota error when pushing Git LFS tracked files. The solution outlines steps to understand the issue, check current LFS usage, review pricing and quotas, and options to resolve the issue, such as purchasing more data packs or optimizing the repository.\n",
              "\n",
              "However, there are a few minor inconsistencies and additions that are not explicitly mentioned in the context:\n",
              "\n",
              "1. The command `git lfs ls-files` to check current LFS usage is not mentioned in the context. While it is a logical step, it is not part of the provided documentation.\n",
              "\n",
              "2. The suggestion to use `.gitignore` to exclude large files is not mentioned in the context. This is a reasonable suggestion but not part of the given information.\n",
              "\n",
              "3. The command `git lfs prune` to clean up the LFS cache is not mentioned in the context. This is a useful suggestion but not part of the provided information.\n",
              "\n",
              "Despite these minor additions, the vast majority of the content in the response is supported by the context. The response does not contain any significant hallucinated or fabricated information that contradicts the context.\n",
              "\n",
              "Therefore, the response is mostly consistent with the provided context, with only minor and inconsequential inconsistencies.\n",
              "\n",
              "__Score:__\n",
              "4"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display the result\n",
        "display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Models\n",
        "\n",
        "`flow-judge` support different model configurations. This refers to the library use for running inference with the models. We currently support:\n",
        "- vLLM sync & async (default engine, mode sync)\n",
        "- Hugging Face\n",
        "- Llamafile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Metrics\n",
        "\n",
        "A judge is initialized with a metric and a model.\n",
        "\n",
        "We include some common metrics in the library, such as:\n",
        "- RESPONSE_FAITHFULNESS_3POINT\n",
        "- RESPONSE_FAITHFULNESS_5POINT\n",
        "- RESPONSE_COMPREHENSIVENESS_3POINT\n",
        "- RESPONSE_COMPREHENSIVENESS_5POINT\n",
        "\n",
        "But you can also implement your own metrics and use them with the judge.\n",
        "\n",
        "Note that metrics have required inputs and outputs as you can see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_columns': ['query', 'context'],\n",
              " 'output_column': 'response',\n",
              " 'expected_output_column': None}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "RESPONSE_FAITHFULNESS_5POINT.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`flow-eval` checks under the hood if the keys match. This is important to ensure the right prompt is being formatted.\n",
        "\n",
        "When you define a custom metric, you should specify the required keys as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running batched evaluations\n",
        "\n",
        "The `LMEvaluator` class also supports batch evaluation. This is useful when you want to evaluate multiple samples at once in Evaluation-Driven Development."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 6/6 [00:08<00:00,  1.48s/it, est. speed input: 871.29 toks/s, output: 175.07 toks/s]\n"
          ]
        }
      ],
      "source": [
        "# Read the sample data\n",
        "import json\n",
        "with open(\"sample_data/csr_assistant.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Create a list of inputs and outputs\n",
        "inputs_batch = [\n",
        "    [\n",
        "        {\"query\": sample[\"query\"]},\n",
        "        {\"context\": sample[\"context\"]},\n",
        "    ]\n",
        "    for sample in data\n",
        "]\n",
        "outputs_batch = [{\"response\": sample[\"response\"]} for sample in data]\n",
        "\n",
        "# Create a list of EvalInput\n",
        "eval_inputs_batch = [EvalInput(inputs=inputs, output=output) for inputs, output in zip(inputs_batch, outputs_batch)]\n",
        "\n",
        "# Run the batch evaluation\n",
        "results = faithfulness_evaluator.batch_evaluate(eval_inputs_batch, save_results=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "__Sample 1:__"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "__Feedback:__\n",
              "The response provided is mostly consistent with the context given. It correctly suggests using Git Large File Storage (LFS) to resolve the issue of pushing large files to a Git repository, which aligns with the context information. The steps outlined for installing and setting up Git LFS are also accurate and relevant to the context.\n",
              "\n",
              "However, there are a few minor inconsistencies and fabrications:\n",
              "\n",
              "1. The response suggests tracking large files using `git lfs track \"*.large-file-extension\"`, but the context does not provide specific instructions for tracking large files.\n",
              "\n",
              "2. The response includes commands like `git add .gitattributes` and `git add large-file.ext`, which are not explicitly mentioned or implied in the context.\n",
              "\n",
              "3. The response assumes the user is working with a specific file extension (`.large-file-extension`), which is not addressed in the context.\n",
              "\n",
              "4. The response mentions pushing changes using `git push origin main`, which is not discussed or implied in the context.\n",
              "\n",
              "While the response is largely based on the context provided, these minor inconsistencies and fabrications prevent it from being completely faithful to the given information.\n",
              "\n",
              "__Score:__\n",
              "3"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "---"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "__Sample 2:__"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "__Feedback:__\n",
              "The response provided is mostly consistent with the given context, but it contains some significant inconsistencies and misleading information. \n",
              "\n",
              "1. The first step is correctly mentioned as checking existing remotes using `git remote -v`, which is consistent with the context.\n",
              "2. The second step suggests changing the URL of the existing origin using `git remote set-url origin new-url`. This is also consistent with the context.\n",
              "3. The third step mentions adding a new remote with a different name using `git remote add new-remote-name new-url`. This is consistent with the context.\n",
              "4. The fourth step is where the response starts to deviate from the context. It suggests removing a remote with a different name and adding a new one using `git remote remove origin` followed by `git remote add origin new-url`. This is not only inconsistent but also potentially harmful advice, as it suggests removing the origin which is the default remote name.\n",
              "\n",
              "Additionally, the response contains misleading information and instructions that are not supported by the context:\n",
              "- It suggests that running `git remote -v` will hide all current remotes, which is incorrect.\n",
              "- It advises to replace 'new-url' with the exact same URL, which contradicts the purpose of changing the URL.\n",
              "- It mentions using 'new-remote-name' with an existing remote, which is not supported by the context.\n",
              "- It suggests that after making these changes, one will definitely encounter the error again, which is not necessarily true and is misleading.\n",
              "\n",
              "Overall, while the response includes some correct information from the context, it also introduces significant misinformation and instructions that deviate from the provided context.\n",
              "\n",
              "__Score:__\n",
              "2"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "---"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "__Sample 3:__"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "__Feedback:__\n",
              "The response provided is mostly consistent with the context given. It accurately describes the `git revert` command and provides a step-by-step guide on how to use it, which aligns well with the context information. The response also mentions the importance of having a backup, which is a good practice as suggested in the context.\n",
              "\n",
              "However, there are a few minor inconsistencies and additions that are not explicitly mentioned in the context:\n",
              "\n",
              "1. The response suggests using `git log` to find the commit hash, which is a reasonable suggestion but not mentioned in the context.\n",
              "2. It provides additional information about reverting multiple commits using a range, which is a useful tip but not explicitly stated in the context.\n",
              "3. The response suggests creating a backup branch, which is good practice but not mentioned in the context.\n",
              "\n",
              "These minor additions and suggestions, while helpful, are not directly supported by the context and could be considered as slight deviations from the strict context provided. Therefore, the response is mostly consistent but not perfectly faithful to the context.\n",
              "\n",
              "__Score:__\n",
              "4"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "---"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "__Sample 4:__"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "__Feedback:__\n",
              "The response provided by the AI system is significantly inconsistent with the given context. The context outlines a detailed, step-by-step process for removing sensitive data from a Git repository, including using tools like BFG Repo-Cleaner or git filter-branch, force-pushing changes to GitHub, contacting GitHub Support, advising collaborators to rebase, and using git commands to remove old references.\n",
              "\n",
              "However, the response suggests that no action is needed, that Git automatically handles the removal of sensitive data, and that no specialized tools or steps are required. This directly contradicts the context, which provides specific tools and steps to follow. The response introduces a substantial amount of fabricated information that deviates from the context, suggesting that the AI system has hallucinated or fabricated details not present in the original context.\n",
              "\n",
              "Therefore, based on the evaluation criteria and scoring rubric, the response is mostly inconsistent with the provided context.\n",
              "\n",
              "__Score:__\n",
              "2"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "---"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "__Sample 5:__"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "__Feedback:__\n",
              "The response is mostly consistent with the provided context, with only minor and inconsequential inconsistencies. The response accurately describes the steps to resolve merge conflicts, including opening conflicted files, identifying conflict markers, deciding which changes to keep, editing the files, staging the resolved files, and committing the changes. It also mentions the option to use `git mergetool` for a visual diff tool, which aligns with the context.\n",
              "\n",
              "However, there are a few minor issues:\n",
              "1. The response suggests using `git add <filename>` instead of `git add <filename>`, which is a minor typographical error.\n",
              "2. The response includes a tip about minimizing merge conflicts by keeping branches up-to-date, which, while helpful, is not part of the original context.\n",
              "\n",
              "These minor issues do not significantly detract from the overall consistency and accuracy of the response with the provided context.\n",
              "\n",
              "__Score:__\n",
              "4"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "---"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "__Sample 6:__"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "__Feedback:__\n",
              "The response provided by the AI system is highly consistent with the context given. It accurately describes the process of adding a remote repository and pushing local changes to a remote repository using Git. The steps outlined in the response are directly supported by the context, which provides the necessary commands and syntax.\n",
              "\n",
              "1. The first step of adding the remote repository is correctly described using the 'git remote add' command, with the correct syntax and example provided.\n",
              "2. The second step of pushing the local branch to the remote repository is accurately described using the 'git push -u origin main' command.\n",
              "\n",
              "There are no hallucinated or fabricated details in the response. All information provided is directly supported by the context. The response is clear, concise, and faithful to the original context, making it easy for the user to follow the instructions.\n",
              "\n",
              "Overall, the response meets the highest standard of consistency and faithfulness to the provided context.\n",
              "\n",
              "__Score:__\n",
              "5"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "---"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualizing the results\n",
        "for i, result in enumerate(results):\n",
        "    display(Markdown(f\"__Sample {i+1}:__\"))\n",
        "    display(Markdown(f\"__Feedback:__\\n{result.feedback}\\n\\n__Score:__\\n{result.score}\"))\n",
        "    display(Markdown(\"---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving the results\n",
        "\n",
        "When running batched evaluation, it's usually recommended to save the results to a file for future reference and reproducibility. This is the default behavior of the evaluate methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 6/6 [00:06<00:00,  1.09s/it, est. speed input: 1180.37 toks/s, output: 204.62 toks/s]\n",
            "INFO:flow_eval.core.base:Saving results to output/\n",
            "INFO:flow_eval.core.io:Results saved to output/Response_Faithfulness_5-point_Likert/results_Response_Faithfulness_5-point_Likert_flowaicomFlow-Judge-v01-AWQ_vllm_2025-01-23T18-29-45.567.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Run the batch evaluation\n",
        "results = faithfulness_evaluator.batch_evaluate(eval_inputs_batch, save_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of output:\n",
            "[PosixPath('output/Response_Faithfulness_5-point_Likert')]\n",
            "\n",
            "Contents of output/Response_Faithfulness_5-point_Likert:\n",
            "[PosixPath('output/Response_Faithfulness_5-point_Likert/results_Response_Faithfulness_5-point_Likert_flowaicomFlow-Judge-v01-AWQ_vllm_2025-01-23T18-29-45.567.jsonl'), PosixPath('output/Response_Faithfulness_5-point_Likert/metadata_Response_Faithfulness_5-point_Likert_flowaicomFlow-Judge-v01-AWQ')]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "output_dir = Path(\"output\")\n",
        "latest_run = next(output_dir.iterdir())\n",
        "\n",
        "print(f\"Contents of {output_dir}:\")\n",
        "print(list(output_dir.iterdir()))\n",
        "\n",
        "print(f\"\\nContents of {latest_run}:\")\n",
        "print(list(latest_run.iterdir()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each evaluation run generates 2 files:\n",
        "- `results_....json`: Contains the evaluation results.\n",
        "- `metadata_....json`: Contains metadata about the evaluation for reproducibility.\n",
        "\n",
        "These files are saved in the `output` directory."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "flow-judge-test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
