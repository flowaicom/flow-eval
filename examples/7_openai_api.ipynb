{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use APIs that are openai-compatible (Together, OpenAI, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.4.0 available.\n",
      "INFO:flow_eval.lm.models.openai:Successfully initialized OpenAI!\n",
      "INFO:flow_eval.lm.models.openai:Initiating batched OpenAI async requests\n",
      "100%|██████████| 6/6 [00:00<00:00, 136770.78it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.together.xyz/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EvalOutput(feedback=\"The response is well-structured and provides a clear step-by-step solution to the customer's issue with pushing large files to their Git repository. The solution is consistent with the provided context, accurately explaining the need for Git Large File Storage (LFS) and providing instructions on how to install and set it up. The response also correctly identifies the limitations of GitHub's file size limit and provides a solution to overcome it. The language used is clear and concise, making it easy to understand for the customer.\\n\\nHowever, the response could be improved by providing more detailed explanations and examples to support the instructions. Additionally, the response could benefit from a more explicit connection between the solution and the customer's issue, making it clearer how the solution addresses the problem.\\n\\nOverall, the response is mostly consistent with the provided context, with only minor inconsistencies and no significant fabrications. The solution is well-structured and easy to follow, making it a good starting point for the customer to resolve their issue.\", score=4), EvalOutput(feedback=\"The generated response is mostly inconsistent with the provided context. While it includes some information from the context, it introduces a substantial amount of hallucinated or fabricated details that deviate from the context. The response provides confusing steps and incorrect information, such as replacing the URL of the existing origin with a random string, which is not supported by the context. Additionally, the response suggests that the user will definitely encounter the 'Remote origin already exists' error after making the changes, which is not accurate. The response also contains a tone that is not helpful or informative, which further detracts from its overall quality.\", score=2), EvalOutput(feedback='The response is well-structured and easy to follow, providing a clear step-by-step guide on how to safely revert a commit in Git. The language used is technical and accurate, demonstrating a good understanding of the context. The response stays within the boundaries of the provided context, using only the information presented to create a solution. The only minor inconsistency is the inclusion of a general reminder to ensure the correct branch is being worked on, which is not directly related to the specific task of reverting a commit. However, this is a minor point and does not detract from the overall quality of the response.\\n\\nThe response is mostly consistent with the provided context, with only minor and inconsequential inconsistencies or fabrications. The vast majority of the content is supported by the context, making it a strong response.', score=4), EvalOutput(feedback=None, score=2), EvalOutput(feedback='The generated response is mostly consistent with the provided context. The response accurately summarizes the steps to resolve merge conflicts in Git, and the language used is clear and concise. The response also includes a helpful tip to minimize merge conflicts in the future. However, the response could be improved by providing more specific examples of how to resolve conflicts, and by including a more detailed explanation of the conflict markers.\\n\\nThe response does not contain any hallucinated or fabricated information that is not supported by the context. The language used is professional and technical, and the response is well-organized and easy to follow.\\n\\nOverall, the response is a good effort, but it could be improved by providing more specific examples and a more detailed explanation of the conflict markers.', score=4), EvalOutput(feedback='The output response is mostly consistent with the provided context. The steps provided to connect a local repository to a new remote repository are accurate and follow the same syntax and structure as the context. The response does not contain any significant amount of hallucinated or fabricated information that contradicts or deviates from the context. However, it could be improved by providing more specific examples or scenarios to illustrate the steps, as the current response is quite general.\\n\\nThe response is faithful to the context, and the majority of the content is supported by the context. The only minor inconsistency is the lack of specific examples, which could be considered a minor fabrication. Overall, the response is well-structured and easy to follow, making it a good starting point for the user.', score=4)]\n"
     ]
    }
   ],
   "source": [
    "from flow_eval import AsyncLMEvaluator\n",
    "from flow_eval.core import EvalInput\n",
    "from flow_eval.lm.metrics import RESPONSE_FAITHFULNESS_5POINT\n",
    "from flow_eval.lm.models import OpenAIModel\n",
    "\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# model = OpenAIModel(exec_async=True)\n",
    "\n",
    "model = OpenAIModel(\n",
    "    _model_id=\"meta-llama/Llama-3-8b-chat-hf\",\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    key_var=\"TOGETHER_API_KEY\",\n",
    "    exec_async=True\n",
    ")\n",
    "\n",
    "# Initialize the judge\n",
    "faithfulness_evaluator = AsyncLMEvaluator(\n",
    "    eval=RESPONSE_FAITHFULNESS_5POINT,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "with open(\"/home/bernardo/repos/flow-eval/examples/sample_data/csr_assistant.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a list of inputs and outputs\n",
    "inputs_batch = [\n",
    "    [\n",
    "        {\"query\": sample[\"query\"]},\n",
    "        {\"context\": sample[\"context\"]},\n",
    "    ]\n",
    "    for sample in data\n",
    "]\n",
    "outputs_batch = [{\"response\": sample[\"response\"]} for sample in data]\n",
    "\n",
    "# Create a list of EvalInput\n",
    "eval_inputs_batch = [EvalInput(inputs=inputs, output=output) for inputs, output in zip(inputs_batch, outputs_batch)]\n",
    "\n",
    "# Run the evaluation\n",
    "async def main():\n",
    "    results = await faithfulness_evaluator.async_batch_evaluate(eval_inputs_batch, save_results=False)\n",
    "    return results\n",
    "\n",
    "results = asyncio.run(main())\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
