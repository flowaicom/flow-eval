{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a Haystack RAG pipeline with FlowJudge\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to evaluate a Retrieval-Augmented Generation (RAG) pipeline built with Haystack using `flow-eval`. We'll showcase how to:\n",
    "\n",
    "1. Set up a basic RAG pipeline using Haystack\n",
    "2. Integrate `flow-eval` evaluators into a Haystack evaluation pipeline\n",
    "3. Assess the RAG system's performance using multiple metrics:\n",
    "   - Semantic Answer Similarity (SAS)\n",
    "   - Context Relevancy\n",
    "   - Faithfulness\n",
    "\n",
    "Key highlights:\n",
    "\n",
    "- Use of the `HaystackLMEvaluator` class to seamlessly incorporate `flow-eval` evaluators\n",
    "- Demonstration of custom metric creation for tailored evaluations\n",
    "- Utilization of both pre-built Haystack evaluators and custom `flow-eval` evaluators\n",
    "\n",
    "By the end of this tutorial, you'll have a clear understanding of how to comprehensively evaluate your Haystack RAG pipelines using `flow-eval`, enabling you to iteratively improve your system's performance without relying on proprietary large language models.\n",
    "\n",
    "### Additional requirements\n",
    "\n",
    "- Haystack: Make sure you have Haystack installed. You can install it via pip:\n",
    "  ```bash\n",
    "  pip install haystack-ai\n",
    "  ```\n",
    "\n",
    "- Sentence Transformers: Make sure you have Sentence Transformers installed. You can install it via pip:\n",
    "  ```bash\n",
    "  pip install \"sentence-transformers>=3.0.0\"\n",
    "  ```\n",
    "\n",
    "- Set your free HuggingFace API token as an environment variable:\n",
    "    ```python\n",
    "    import os\n",
    "    os.environ[\"HF_TOKEN\"] = \"your_token_here\"\n",
    "    ```\n",
    "\n",
    "    You can get your HuggingFace API token [here](https://huggingface.co/settings/tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this notebook primarily demonstrates the integration of `flow-eval` with Haystack for evaluating RAG pipelines. While we do set up a basic RAG pipeline using Haystack, the main emphasis is on the evaluation process using `flow-eval` evaluators.\n",
    "\n",
    "For detailed explanations on building RAG pipelines with Haystack, please refer to the official [Haystack documentation](https://docs.haystack.deepset.ai/docs/intro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this tutorial, we are going to use a subset of the `LegalBench` dataset, which contains contracts and questions from the contracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from datasets import Dataset\n",
    "except ImportError as e:\n",
    "    print(\"datasets is not installed. \")\n",
    "    print(\"Please run `pip install datasets` to install it.\")\n",
    "    print(\"\\nAfter installation, restart the kernel and run this cell again.\")\n",
    "    raise SystemExit(f\"Stopping execution due to missing datasets dependency: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"flowaicom/legalbench_contracts_qa_subset\", \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains:\n",
    "- Questions: A question about the contract.\n",
    "- Context: The contract itself.\n",
    "- Original answer: The original answer to the question which can be considered as the ground truth.\n",
    "- Answer: The answer used for generating the answer with reasoning, which can include noise with respect to the original answer.\n",
    "- Answer with reasoning: An answer to the question including the reasoning for the answer based on the contract.\n",
    "\n",
    "For this tutorial:\n",
    "- We use instances without perturbations (where original_answer == answer)\n",
    "- The contract text (context) is used to create documents\n",
    "- We use `answer_with_reasoning` as the ground truth for evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from haystack import Document\n",
    "except ImportError:\n",
    "    print(\"Haystack is not installed. \")\n",
    "    print(\"Please install it according to the 'Additional Requirements' section above.\")\n",
    "    print(\"\\nAfter installation, restart the kernel and run this cell again.\")\n",
    "    raise SystemExit(\"Stopping execution due to missing Haystack dependency.\")\n",
    "\n",
    "filtered_ds = ds.filter(lambda x: x['original_answer'] == x['answer'])\n",
    "\n",
    "all_documents = [Document(content=context) for context in filtered_ds['train']['context']]\n",
    "all_questions = [q for q in filtered_ds['train']['question']]\n",
    "all_ground_truths = [a for a in filtered_ds['train']['answer_with_reasoning']]\n",
    "\n",
    "print(f\"Number of documents: {len(all_documents)}\")\n",
    "print(f\"Number of questions: {len(all_questions)}\")\n",
    "print(f\"Number of ground truths: {len(all_ground_truths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(f\"**Question:** {all_questions[0]}\"))\n",
    "display(Markdown(f\"**Context:** {all_documents[0].content}\"))\n",
    "display(Markdown(f\"**Ground truth answer:** {all_ground_truths[0]}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a RAG pipeline with Haystack\n",
    "\n",
    "We will be creating a very simple RAG pipeline with Haystack.\n",
    "\n",
    "For more detail explanations about building the RAG pipeline, please refer to this tutorial in the Haystack documentation - [Tutorial: Evaluating RAG pipelines](https://haystack.deepset.ai/tutorials/35_evaluating_rag_pipelines)\n",
    "\n",
    ">Note that we have made minor modifications to the pipeline for this tutorial. In particular, we are using `HuggingFaceAPIChatGenerator` and `ChatPromptBuilder`.\n",
    "\n",
    "### Indexing the documents\n",
    "\n",
    "We need to index the documents so we can later use a retriever to find the most similar document to the question.\n",
    "\n",
    "We are using the `InMemoryDocumentStore`, which is a simple in-memory document store that doesn't require setting up a database.\n",
    "\n",
    "We are also using an small open-source embedding model from Sentence Transformers to convert the documents into embeddings.\n",
    "\n",
    "Finally, we are using the `DocumentWriter` to write the documents into the document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "document_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "document_writer = DocumentWriter(document_store=document_store, policy=DuplicatePolicy.SKIP)\n",
    "\n",
    "indexing = Pipeline()\n",
    "indexing.add_component(instance=document_embedder, name=\"document_embedder\")\n",
    "indexing.add_component(instance=document_writer, name=\"document_writer\")\n",
    "\n",
    "indexing.connect(\"document_embedder.documents\", \"document_writer.documents\")\n",
    "\n",
    "indexing.run({\"document_embedder\": {\"documents\": all_documents}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the RAG pipeline\n",
    "\n",
    "Haystack lets us easily create a RAG pipeline using:\n",
    "\n",
    "- `InMemoryEmbeddingRetriever` which will get the relevant documents to the query.\n",
    "- `HuggingFaceAPIChatGenerator` to generate the answer to the question. We are going to use a small open model for this example.\n",
    "\n",
    ">Note you can use the free serverless inference API from HuggingFace to quickly experiment with different models. However, it's rate-limited and not suitable for production. To make use of the API, you just need to provide [your free HuggingFace API token](https://huggingface.co/settings/tokens).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.builders import AnswerBuilder, ChatPromptBuilder\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.generators.chat import HuggingFaceAPIChatGenerator\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.utils.hf import HFGenerationAPIType, Secret\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "api_type = HFGenerationAPIType.SERVERLESS_INFERENCE_API\n",
    "llm = HuggingFaceAPIChatGenerator(api_type=api_type,\n",
    "                                        api_params={\"model\": \"microsoft/Phi-3.5-mini-instruct\"},\n",
    "                                        token=Secret.from_env_var(\"HF_TOKEN\")\n",
    "                                        )\n",
    "\n",
    "\n",
    "template_str = \"\"\"\n",
    "You have to answer the following question based on the given context information only.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "template = [ChatMessage.from_user(template_str)]\n",
    "prompt_builder = ChatPromptBuilder(template=template)\n",
    "\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\n",
    "    \"query_embedder\", SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    ")\n",
    "rag_pipeline.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store, top_k=3))\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", llm)\n",
    "rag_pipeline.add_component(\"answer_builder\", AnswerBuilder())\n",
    "\n",
    "rag_pipeline.connect(\"query_embedder\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "rag_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
    "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the pipeline with a single question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of the pipeline\n",
    "question = \"Does CNN permit using bots to artificially increase page visits for certain content?\"\n",
    "\n",
    "response = rag_pipeline.run(\n",
    "    {\n",
    "        \"query_embedder\": {\"text\": question},\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "        \"answer_builder\": {\"query\": question},\n",
    "    }\n",
    ")\n",
    "print(response[\"answer_builder\"][\"answers\"][0].data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the retrieved documents and similarity scores\n",
    "for i, doc in enumerate(response['answer_builder']['answers'][0].documents, 1):\n",
    "    display(Markdown(f\"\"\"**Document {i} (Score: {doc.score:.4f}):**\\n\\n{doc.content[:500]}...\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the pipeline\n",
    "\n",
    "With our initial RAG pipeline prototype in place, we can now focus on evaluation.\n",
    "\n",
    "To showcase the integration of `FlowJudge` within the Haystack framework, we'll evaluate the pipeline using both statistical and model-based evaluators.\n",
    "\n",
    "Haystack employs the concept of an __Evaluation pipeline__, which computes scoring metrics to assess the RAG pipeline's performance.\n",
    "\n",
    "Our evaluation pipeline will incorporate three key metrics:\n",
    "- __Semantic Answer Similarity (SAS)__: Measures the semantic similarity between generated and ground truth answers, going beyond simple lexical matching.\n",
    "- __Context Relevancy__: Determines how well the retrieved documents align with the given query.\n",
    "- __Faithfulness__: Assesses the extent to which the generated answer is grounded in the retrieved documents.\n",
    "\n",
    "For context relevancy and faithfulness, we'll leverage `FlowJudge` evaluators, eliminating the need for proprietary large models like GPT-4 or Claude 3.5 Sonnet.\n",
    "\n",
    "### Obtaining generated answers\n",
    "\n",
    "Our first step is to generate answers using the RAG pipeline.\n",
    "\n",
    ">Note: We're using HuggingFace's free serverless inference API, which may take several minutes. To avoid rate limits, we're processing only 20 questions. If execution fails, you can resume from the last successful point by rerunning the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = all_questions[:20]\n",
    "ground_truths = all_ground_truths[:20]\n",
    "\n",
    "rag_answers = []\n",
    "retrieved_docs = []\n",
    "\n",
    "for question in questions:\n",
    "    response = rag_pipeline.run(\n",
    "        {\n",
    "            \"query_embedder\": {\"text\": question},\n",
    "            \"prompt_builder\": {\"question\": question},\n",
    "            \"answer_builder\": {\"query\": question},\n",
    "        }\n",
    "    )\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"Answer from pipeline:\")\n",
    "    print(response[\"answer_builder\"][\"answers\"][0].data)\n",
    "    print(\"\\n-----------------------------------\\n\")\n",
    "\n",
    "    rag_answers.append(response[\"answer_builder\"][\"answers\"][0].data)\n",
    "    retrieved_docs.append(response[\"answer_builder\"][\"answers\"][0].documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now convert the retrieved documents into a single string so `FlowJudge` can format the prompt properly under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the retrieved documents into a single string\n",
    "str_retrieved_docs = []\n",
    "for docs in retrieved_docs:\n",
    "    str_retrieved_doc = \"\"\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        str_retrieved_doc += doc.content\n",
    "        str_retrieved_doc += \"\\n\"\n",
    "    str_retrieved_docs.append(str_retrieved_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"**Retrieved documents:** {str_retrieved_docs[0]}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluators in Haystack\n",
    "\n",
    "__Evaluators__ in Haystack are versatile components that can operate independently or as integral parts of a pipeline.\n",
    "\n",
    "We'll construct an evaluation pipeline to efficiently obtain scores from all evaluators in a single pass. Additionally, Haystack provides functionality to generate a comprehensive evaluation report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating flow-eval evaluators using the HaystackFlowJudge class\n",
    "\n",
    "We can use our integration with Haystack to create `flow-eval` evaluators in a flexible way. The process is as follows:\n",
    "1. Create a `LMEval` that will be used to compute the score for the evaluator.\n",
    "2. Initialize the model - We are using the transformers configuration for Flow-Judge-v0.1.\n",
    "3. Instantiate the `HaystackLMEvaluator` evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important Note on Model Selection:**\n",
    "> \n",
    "> There's a known issue with Phi-3 models producing gibberish outputs for contexts exceeding 4096 tokens (including input and output). While this has been addressed in recent transformers library updates, still remains an issue in the vLLM engine. We recommend the following:\n",
    "> \n",
    "> - For longer contexts: Use the `Flow-Judge-v0.1_HF` model configuration.\n",
    "> - **Caveat:** Inference with transformers is significantly slower than with optimized runtimes.\n",
    "> \n",
    "> This approach ensures reliable outputs for extensive contexts, albeit with a trade-off in processing speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_eval.integrations.haystack import HaystackEvaluator\n",
    "from flow_eval.lm import LMEval, RubricItem\n",
    "from flow_eval.lm.metrics import RESPONSE_FAITHFULNESS_5POINT\n",
    "from flow_eval.lm.models import Vllm, Hf, Llamafile, Baseten\n",
    "\n",
    "# Create a model using Hugging Face Transformers with Flash Attention\n",
    "model = Hf()\n",
    "\n",
    "# Or if not running on Ampere GPU or newer, create a model using no flash attn and Hugging Face Transformers\n",
    "# model = Hf(flash_attn=False)\n",
    "\n",
    "# Creating a model using Vllm\n",
    "# model = Vllm()\n",
    "\n",
    "# If you have other applications open taking up VRAM, you can use less VRAM by setting gpu_memory_utilization to a lower value.\n",
    "# model = Vllm(gpu_memory_utilization=0.70)\n",
    "\n",
    "# If you are running on a Silicon Mac, you can create a model using Llamafile\n",
    "# model = Llamafile()\n",
    "\n",
    "# Or create a model using Baseten if you don't want to run locally.\n",
    "# As a pre-requisite step:\n",
    "#  - Sign up to Baseten\n",
    "#  - Generate an api key https://app.baseten.co/settings/api_keys\n",
    "#  - Set the api key as an environment variable & initialize:\n",
    "# import os\n",
    "# os.environ[\"BASETEN_API_KEY\"] = \"your_api_key\"\n",
    "# model = Baseten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the context relevancy metric from scratch. For learning more about how to create custom metrics, refer to the [custom metrics tutorial](https://github.com/flowaicom/flow-judge/blob/main/examples/2_custom_evaluation_criteria.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context relevancy\n",
    "cr_criteria = \"Based on the provided query and context, how relevant and sufficient is the context for responding to the query?\"\n",
    "cr_rubric = [\n",
    "    RubricItem(\n",
    "        score=1,\n",
    "        description=\"The context provided is not relevant or insufficient to respond to the query.\"\n",
    "    ),\n",
    "    RubricItem(\n",
    "        score=2,\n",
    "        description=\"The context is mostly irrelevant to the query. It may contain some tangentially related information but is insufficient for adequately responding to the query.\"\n",
    "    ),\n",
    "    RubricItem(\n",
    "        score=3,\n",
    "        description=\"The context is somewhat relevant to the query. It contains some information that could be used to partially respond to the query, but key details are missing for a complete response.\"\n",
    "    ),\n",
    "    RubricItem(\n",
    "        score=4,\n",
    "        description=\"The context is mostly relevant to the query. It contains most of the necessary information to respond to the query, but may be missing some minor details.\"\n",
    "    ),\n",
    "    RubricItem(\n",
    "        score=5,\n",
    "        description=\"The context is highly relevant to the query. It contains all the necessary information to comprehensively respond to the query without needing any additional context.\"\n",
    "    )\n",
    "]\n",
    "cr_eval = LMEval(\n",
    "    name=\"Context Relevancy\",\n",
    "    criteria=cr_criteria,\n",
    "    rubric=cr_rubric,\n",
    "    input_columns=[\"question\"],\n",
    "    output_column=\"contexts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating the faithfulness evaluator, we are going to use the `RESPONSE_FAITHFULNESS_5POINT` preset in flow-judge library as a template.\n",
    "\n",
    "> Note that we need to use the expected keys so we need to update required inputs and outputs to match the expected keys in the RAG pipeline. In this case, the score descriptions are still relevant with these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_criteria = RESPONSE_FAITHFULNESS_5POINT.criteria\n",
    "ff_rubric = RESPONSE_FAITHFULNESS_5POINT.rubric\n",
    "\n",
    "display(Markdown(f\"**Criteria:** {ff_criteria}\"))\n",
    "display(Markdown(\"**Rubric:**\"))\n",
    "\n",
    "for item in ff_rubric:\n",
    "    display(Markdown(f\"- **Score {item.score}:** {item.description}\"))\n",
    "\n",
    "ff_eval = LMEval(\n",
    "    name=\"Faithfulness\",\n",
    "    criteria=ff_criteria,\n",
    "    rubric=ff_rubric,\n",
    "    input_columns=[\"question\", \"contexts\"],\n",
    "    output_column=\"predicted_answers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the Flow Judge evaluators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_evaluator = HaystackLMEvaluator(\n",
    "    eval=cr_eval,\n",
    "    model=model, # the vLLM instance of Flow-Judge-v0.1\n",
    "    progress_bar=True,\n",
    "    raise_on_failure=True, # to raise an error when pipeline run fails\n",
    "    save_results=True, # to save evaluation results to disk\n",
    "    fail_on_parse_error=False # to fail if there is a parsing error, otherwise return \"Error\" and score -1\n",
    ")\n",
    "\n",
    "ff_evaluator = HaystackLMEvaluator(\n",
    "    eval=ff_eval,\n",
    "    model=model,\n",
    "    progress_bar=True,\n",
    "    raise_on_failure=True,\n",
    "    save_results=True,\n",
    "    fail_on_parse_error=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Haystack evaluators\n",
    "\n",
    "Now let's crete the semantic answer similarity evaluator using the Haystack implementation. This evaluator will use the same embedding model as the retriever in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.evaluators.sas_evaluator import SASEvaluator\n",
    "\n",
    "sas_evaluator = SASEvaluator(model=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation pipeline\n",
    "\n",
    "We can now create a Haystack evaluation pipeline that will evaluate the RAG pipeline and obtains the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pipeline = Pipeline()\n",
    "\n",
    "# add components to the pipeline\n",
    "eval_pipeline.add_component(\"sas_evaluator\", sas_evaluator)\n",
    "eval_pipeline.add_component(\"cr_evaluator\", cr_evaluator)\n",
    "eval_pipeline.add_component(\"ff_evaluator\", ff_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note that executing the following cell might take a while to complete due to the size of the inputs, specially if running on a machine with low resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = eval_pipeline.run(\n",
    "    {\n",
    "        \"sas_evaluator\": {\n",
    "            'predicted_answers': rag_answers,\n",
    "            'ground_truth_answers': ground_truths,\n",
    "        },\n",
    "        \"cr_evaluator\": {\n",
    "            'question': questions,\n",
    "            'contexts': str_retrieved_docs,\n",
    "        },\n",
    "        \"ff_evaluator\": {\n",
    "            'question': questions,\n",
    "            'contexts': str_retrieved_docs,\n",
    "            'predicted_answers': rag_answers,\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation report\n",
    "\n",
    "Haystack provides a convenient way to generate an evaluation report using the `EvaluationRunResult` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.evaluation.eval_run_result import EvaluationRunResult\n",
    "\n",
    "inputs = {\n",
    "    \"question\": questions,\n",
    "    \"contexts\": str_retrieved_docs,\n",
    "    \"answer\": ground_truths,\n",
    "    \"predicted_answer\": rag_answers,\n",
    "}\n",
    "\n",
    "evaluation_result = EvaluationRunResult(run_name=\"report\", inputs=inputs, results=results)\n",
    "evaluation_result.score_report()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily conver to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = evaluation_result.to_pandas()\n",
    "results_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we demonstrated how to evaluate a Retrieval-Augmented Generation (RAG) pipeline using `flow-eval` and Haystack. Key aspects covered include:\n",
    "\n",
    "1. Setting up a basic RAG pipeline with Haystack:\n",
    "   - Using `InMemoryDocumentStore` for document storage\n",
    "   - Implementing `SentenceTransformersDocumentEmbedder` for document embedding\n",
    "   - Utilizing `HuggingFaceAPIChatGenerator` for answer generation\n",
    "\n",
    "2. Creating custom evaluators with `FlowJudge`:\n",
    "   - Developing a custom metric for context relevancy\n",
    "   - Adapting a preset metric for faithfulness\n",
    "   - Using the `HaystackFlowJudge` class to integrate FlowJudge evaluators into Haystack\n",
    "\n",
    "3. Building a comprehensive evaluation pipeline:\n",
    "   - Incorporating both FlowJudge and native Haystack evaluators\n",
    "   - Using `SASEvaluator` for semantic answer similarity\n",
    "\n",
    "4. Executing the evaluation and analyzing results:\n",
    "   - Running the evaluation pipeline on a subset of questions\n",
    "   - Utilizing `EvaluationRunResult` to generate a summary report\n",
    "   - Converting results to a pandas DataFrame for further analysis\n",
    "\n",
    "5. Demonstrating the flexibility of `flow-eval`:\n",
    "   - Seamless integration with Haystack's evaluation framework\n",
    "   - Ability to create custom metrics and adapt existing ones\n",
    "   - Using open-source models to avoid reliance on proprietary large language models\n",
    "\n",
    "This tutorial showcases how `flow-eval` can be effectively used to evaluate and iteratively improve RAG pipelines built with Haystack, providing a comprehensive assessment of performance across multiple dimensions including semantic similarity, context relevancy, and faithfulness.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow-judge-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
