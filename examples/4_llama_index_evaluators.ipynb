{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using flow-eval with Llama Index evaluators\n",
    "\n",
    "## Introduction to flow-eval and Llama Index Integration\n",
    "\n",
    "Flow-Judge-v0.1 is an open-source language model optimized for evaluating AI systems. This tutorial demonstrates how to integrate `flow-eval` with Llama Index evaluators. By the end of this notebook, you'll understand how to create custom evals, run evaluations, and analyze results using both flow-eval and Llama Index tools.\n",
    "\n",
    "This notebook is inspired by the [prometheus_evaluation.ipynb](https://github.com/run-llama/llama_index/blob/9083c6d199443076bc9d764022d4c98260d8e504/docs/docs/examples/evaluation/prometheus_evaluation.ipynb) example.\n",
    "\n",
    "\n",
    "## `Flow-Judge-v0.1`\n",
    "\n",
    "`Flow-Judge-v0.1` is an open-source, lightweight (3.8B) language model optimized for LLM system evaluations. Crafted for accuracy, speed, and customization.\n",
    "\n",
    "Read the technical report [here](https://www.flow-ai.com/blog/flow-judge).\n",
    "\n",
    "## Llama Index evaluators\n",
    "\n",
    "Llama Index is a powerful framework for building LLM applications, that offers key modules to measure the quality of generated results as well as retrieval quality.\n",
    "\n",
    "Refer to the [Llama Index documentation](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/) for more information.\n",
    "\n",
    "LlamaIndex offers LLM-based evaluation modules to measure the quality of results. This uses a \"gold\" LLM (e.g. GPT-4) to decide whether the predicted answer is correct in a variety of ways.\n",
    "\n",
    "In this notebook, we will make use of `Flow-Judge-v0.1` to evaluate the quality of the results generated by a RAG system, instead of reference evaluators like GPT-4o or Claude 3.5 Sonnet.\n",
    "\n",
    "### Additional requirements\n",
    "\n",
    "- llama-index: Make sure you have Llama Index installed. You can install it via pip:\n",
    "  ```bash\n",
    "  pip install llama-index\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from llama_index.core.evaluation import BaseEvaluator\n",
    "except ImportError:\n",
    "    print(\"Llama Index is not installed. \")\n",
    "    print(\"Please install it according to the 'Additional Requirements' section above.\")\n",
    "    print(\"\\nAfter installation, restart the kernel and run this cell again.\")\n",
    "    raise SystemExit(\"Stopping execution due to missing Llama Index dependency.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API key\n",
    "\n",
    "You need to provide an OpenAI API key to use the Llama Index evaluator with gpt-4o and also generating the responses.\n",
    "\n",
    "We limited the number of requests to avoid high costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "For this tutorial, we are going to use the quantized version of `Flow-Judge-v0.1`. Under the hood, `flow-judge` uses the vLLM engine to run the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.4.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:38:39 awq_marlin.py:90] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "WARNING 01-23 19:38:39 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 01-23 19:38:39 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='flowaicom/Flow-Judge-v0.1-AWQ', speculative_config=None, tokenizer='flowaicom/Flow-Judge-v0.1-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=flowaicom/Flow-Judge-v0.1-AWQ, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 01-23 19:38:39 model_runner.py:1014] Starting to load model flowaicom/Flow-Judge-v0.1-AWQ...\n",
      "INFO 01-23 19:38:40 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "INFO 01-23 19:38:40 weight_utils.py:287] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8587d5f10eae4bf8854b63fcd794b554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:38:41 model_runner.py:1025] Loading model weights took 2.1717 GB\n",
      "INFO 01-23 19:38:43 gpu_executor.py:122] # GPU blocks: 3086, # CPU blocks: 682\n"
     ]
    }
   ],
   "source": [
    "from flow_eval.lm.models import Vllm #, Llamafile, Hf\n",
    "# from flow_eval import Baseten\n",
    "\n",
    "# If you are running on an Ampere GPU or newer, create a model using VLLM\n",
    "model = Vllm(exec_async=True)\n",
    "\n",
    "# Or if not running on Ampere GPU or newer, create a model using no flash attn and Hugging Face Transformers\n",
    "# model = Hf(flash_attn=False)\n",
    "\n",
    "# Or create a model using Llamafile if not running an Nvidia GPU & running a Silicon MacOS for example\n",
    "# model = Llamafile()\n",
    "\n",
    "# Or create a model using Baseten if you don't want to run locally.\n",
    "# As a pre-requisite step:\n",
    "#  - Sign up to Baseten\n",
    "#  - Generate an api key https://app.baseten.co/settings/api_keys\n",
    "#  - Set the api key as an environment variable & initialize:\n",
    "# import os\n",
    "# os.environ[\"BASETEN_API_KEY\"] = \"your_api_key\"\n",
    "# model = Baseten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We need to select the asynchronous version of the Flow Judge model to ensure compatibility with Llama Index's BaseEvaluator class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correctness evaluation\n",
    "\n",
    "The Llama Index `CorrectnessEvaluator` evaluates the correctness of a question answering system.\n",
    "\n",
    "The evaluator depends on a `reference` answer to be provided, in addition to the query string and response string.\n",
    "\n",
    "It grades the response based on the reference answer, outputting a score between 1 and 5, where 1 is the worst and 5 is the best, along with a reasoning for the score.\n",
    "\n",
    "Let's see how we can create this same evaluator using `flow-eval`'s Llama Index integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For this demonstration, let's create a single instance to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Analyze the impact of the Industrial Revolution on urbanization in 19th century England, focusing on demographic shifts, living conditions, and social reforms. Include specific examples and statistics to support your analysis.\"\"\"\n",
    "reference = \"\"\"The Industrial Revolution in England during the 19th century had a profound impact on urbanization, resulting in significant demographic shifts, changes in living conditions, and the emergence of social reforms.\n",
    "\n",
    "1. Demographic Shifts:\n",
    "   - Rural-to-urban migration: Between 1811 and 1851, the urban population in England and Wales increased from 35% to 54%.\n",
    "   - Population growth: London's population grew from 1 million in 1800 to 6.7 million by 1900.\n",
    "   - Example: Manchester's population increased tenfold, from 75,000 in 1801 to 750,000 by 1901.\n",
    "\n",
    "2. Living Conditions:\n",
    "   - Overcrowding: In 1851, the average number of people per house in Liverpool was 7.6, compared to 5.5 in rural areas.\n",
    "   - Poor sanitation: In 1842, only 5% of working-class homes in Manchester had drainage systems.\n",
    "   - High mortality rates: Life expectancy in industrial cities was 29 years, compared to 45 years in rural areas.\n",
    "   - Example: The cholera epidemic of 1832 killed over 20,000 people in England, with a disproportionate impact on urban areas.\n",
    "\n",
    "3. Social Reforms:\n",
    "   - Public Health Act of 1848: Established local boards of health to improve sanitation and water supply.\n",
    "   - Factory Acts (1833, 1844, 1847): Regulated working conditions and hours, especially for women and children.\n",
    "   - Education Act of 1870: Introduced compulsory elementary education.\n",
    "   - Example: The creation of model towns like Saltaire (1851) and Bournville (1879) aimed to provide better living conditions for workers.\n",
    "\n",
    "In conclusion, the Industrial Revolution led to rapid urbanization in 19th century England, causing significant demographic shifts and initially worsening living conditions. However, these challenges ultimately spurred social reforms that aimed to improve the quality of life for urban residents.\"\"\"\n",
    "response = \"\"\"The Industrial Revolution in 19th century England significantly influenced urbanization, causing substantial changes in demographics, living conditions, and social structures.\n",
    "\n",
    "1. Demographic Shifts:\n",
    "   - There was a notable rural-to-urban migration, with the urban population in England and Wales increasing from 35% to 60% between 1811 and 1851.\n",
    "   - Cities grew rapidly. For example, London's population expanded from 1 million in 1800 to 5.5 million by 1900.\n",
    "   - Manchester's population increased from 75,000 in 1801 to 500,000 by 1901.\n",
    "\n",
    "2. Living Conditions:\n",
    "   - Overcrowding was a major issue. In Liverpool, the average number of people per house in 1851 was 8.2, compared to 6.0 in rural areas.\n",
    "   - Sanitation was poor, with only 10% of working-class homes in Manchester having drainage systems in 1842.\n",
    "   - Health problems were widespread. Life expectancy in industrial cities dropped to 25 years, while it remained at 40 years in rural areas.\n",
    "   - The cholera epidemic of 1832 exemplifies the health crisis, killing over 30,000 people in England, primarily in urban areas.\n",
    "\n",
    "3. Social Reforms:\n",
    "   - The Public Health Act of 1850 was introduced to improve sanitation and water supply in urban areas.\n",
    "   - Factory Acts were passed in 1833 and 1845 to regulate working conditions, particularly for women and children.\n",
    "   - The Education Act of 1875 made elementary education compulsory, addressing the need for a more educated workforce.\n",
    "   - Some industrialists created model towns, such as New Lanark (1851) and Port Sunlight (1879), to provide better living conditions for workers.\n",
    "\n",
    "These changes transformed England's urban landscape, creating challenges that eventually led to social and legislative reforms aimed at improving the quality of life for city dwellers. However, the full impact of these reforms wasn't felt until the early 20th century.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the correctness metric\n",
    "from flow_eval.lm import LMEval, RubricItem\n",
    "\n",
    "evaluation_criteria = \"\"\"Is the generated answer relevant to the user query and reference answer?\"\"\"\n",
    "\n",
    "rubric = [\n",
    "    RubricItem(\n",
    "        score=1,\n",
    "        description=\"The generated answer is not relevant to the user query and reference answer.\"\n",
    "    ),\n",
    "    RubricItem(\n",
    "        score=2,\n",
    "        description=\"The generated answer is according to reference answer but not relevant to user query.\"\n",
    "    ),\n",
    "    RubricItem(\n",
    "        score=3,\n",
    "        description=\"The generated answer is relevant to the user query and reference answer but contains mistakes.\"\n",
    "    ),\n",
    "    RubricItem(\n",
    "        score=4,\n",
    "        description=\"The generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.\"\n",
    "    ),\n",
    "    RubricItem(\n",
    "        score=5,\n",
    "        description=\"The generated answer is relevant to the user query and fully correct according to the reference answer.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "required_inputs = [\"query\", \"reference\"]\n",
    "required_output = \"response\"\n",
    "\n",
    "correctness_eval = LMEval(\n",
    "    name=\"correctness\",\n",
    "    criteria=evaluation_criteria,\n",
    "    rubric=rubric,\n",
    "    input_columns=required_inputs,\n",
    "    output_column=required_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LMEval(name='correctness', input_columns=['query', 'reference'], output_column='response', expected_output_column=None, criteria='Is the generated answer relevant to the user query and reference answer?', rubric=[RubricItem(score=1, description='The generated answer is not relevant to the user query and reference answer.'), RubricItem(score=2, description='The generated answer is according to reference answer but not relevant to user query.'), RubricItem(score=3, description='The generated answer is relevant to the user query and reference answer but contains mistakes.'), RubricItem(score=4, description='The generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.'), RubricItem(score=5, description='The generated answer is relevant to the user query and fully correct according to the reference answer.')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined our correctness metric, we can easily create our Flow Judge evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_eval.integrations.llama_index import LlamaIndexLMEvaluator\n",
    "\n",
    "flow_eval_correctness_evaluator = LlamaIndexLMEvaluator(\n",
    "    model=model,\n",
    "    eval=correctness_eval\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate our response using the `evaluate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:39:59 async_llm_engine.py:204] Added request req_755540992.\n",
      "INFO 01-23 19:39:59 metrics.py:351] Avg prompt throughput: 23.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-23 19:40:04 metrics.py:351] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 46.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-23 19:40:07 async_llm_engine.py:172] Finished request req_755540992.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:40:07 async_llm_engine.py:216] Aborted request req_755540992.\n",
      "INFO 01-23 19:40:48 metrics.py:351] Avg prompt throughput: 20.7 tokens/s, Avg generation throughput: 2.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-23 19:40:53 metrics.py:351] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 46.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-23 19:40:54 async_llm_engine.py:172] Finished request req_756117248.\n",
      "INFO 01-23 19:42:13 metrics.py:351] Avg prompt throughput: 92.5 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-23 19:42:18 metrics.py:351] Avg prompt throughput: 1165.6 tokens/s, Avg generation throughput: 175.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 28.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-23 19:42:18 async_llm_engine.py:172] Finished request req_792692128.\n",
      "INFO 01-23 19:42:18 async_llm_engine.py:172] Finished request req_799038176.\n",
      "INFO 01-23 19:42:19 async_llm_engine.py:172] Finished request req_797083584.\n",
      "INFO 01-23 19:42:20 async_llm_engine.py:172] Finished request req_798077456.\n",
      "INFO 01-23 19:42:20 async_llm_engine.py:172] Finished request req_799019888.\n",
      "INFO 01-23 19:42:21 async_llm_engine.py:172] Finished request req_798271568.\n",
      "INFO 01-23 19:42:21 async_llm_engine.py:172] Finished request req_797697424.\n",
      "INFO 01-23 19:42:23 async_llm_engine.py:172] Finished request req_797235712.\n",
      "INFO 01-23 19:42:23 metrics.py:351] Avg prompt throughput: 2169.3 tokens/s, Avg generation throughput: 142.1 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 23.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-23 19:42:23 async_llm_engine.py:172] Finished request req_756050224.\n",
      "INFO 01-23 19:42:24 async_llm_engine.py:172] Finished request req_803260048.\n",
      "INFO 01-23 19:42:25 async_llm_engine.py:172] Finished request req_802282352.\n",
      "INFO 01-23 19:42:27 async_llm_engine.py:172] Finished request req_798263936.\n",
      "INFO 01-23 19:42:28 metrics.py:351] Avg prompt throughput: 1460.0 tokens/s, Avg generation throughput: 125.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 21.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-23 19:42:30 async_llm_engine.py:172] Finished request req_798077456.\n",
      "INFO 01-23 19:42:32 async_llm_engine.py:172] Finished request req_803209440.\n",
      "INFO 01-23 19:42:32 async_llm_engine.py:172] Finished request req_802194064.\n",
      "INFO 01-23 19:42:32 async_llm_engine.py:172] Finished request req_797267184.\n",
      "INFO 01-23 19:42:33 async_llm_engine.py:172] Finished request req_805902112.\n",
      "INFO 01-23 19:42:33 metrics.py:351] Avg prompt throughput: 147.1 tokens/s, Avg generation throughput: 179.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-23 19:42:34 async_llm_engine.py:172] Finished request req_800031680.\n",
      "INFO 01-23 19:42:34 async_llm_engine.py:172] Finished request req_799015216.\n",
      "INFO 01-23 19:42:35 async_llm_engine.py:172] Finished request req_798349632.\n"
     ]
    }
   ],
   "source": [
    "result = flow_eval_correctness_evaluator.evaluate(\n",
    "    query=query,\n",
    "    reference=reference,\n",
    "    response=response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationResult(query='Analyze the impact of the Industrial Revolution on urbanization in 19th century England, focusing on demographic shifts, living conditions, and social reforms. Include specific examples and statistics to support your analysis.', contexts=None, response=\"The Industrial Revolution in 19th century England significantly influenced urbanization, causing substantial changes in demographics, living conditions, and social structures.\\n\\n1. Demographic Shifts:\\n   - There was a notable rural-to-urban migration, with the urban population in England and Wales increasing from 35% to 60% between 1811 and 1851.\\n   - Cities grew rapidly. For example, London's population expanded from 1 million in 1800 to 5.5 million by 1900.\\n   - Manchester's population increased from 75,000 in 1801 to 500,000 by 1901.\\n\\n2. Living Conditions:\\n   - Overcrowding was a major issue. In Liverpool, the average number of people per house in 1851 was 8.2, compared to 6.0 in rural areas.\\n   - Sanitation was poor, with only 10% of working-class homes in Manchester having drainage systems in 1842.\\n   - Health problems were widespread. Life expectancy in industrial cities dropped to 25 years, while it remained at 40 years in rural areas.\\n   - The cholera epidemic of 1832 exemplifies the health crisis, killing over 30,000 people in England, primarily in urban areas.\\n\\n3. Social Reforms:\\n   - The Public Health Act of 1850 was introduced to improve sanitation and water supply in urban areas.\\n   - Factory Acts were passed in 1833 and 1845 to regulate working conditions, particularly for women and children.\\n   - The Education Act of 1875 made elementary education compulsory, addressing the need for a more educated workforce.\\n   - Some industrialists created model towns, such as New Lanark (1851) and Port Sunlight (1879), to provide better living conditions for workers.\\n\\nThese changes transformed England's urban landscape, creating challenges that eventually led to social and legislative reforms aimed at improving the quality of life for city dwellers. However, the full impact of these reforms wasn't felt until the early 20th century.\", passing=None, feedback='The generated response is highly relevant to the user query and reference answer. It accurately addresses the impact of the Industrial Revolution on urbanization in 19th century England, focusing on demographic shifts, living conditions, and social reforms.\\n\\n1. **Demographic Shifts**: The response correctly highlights the rural-to-urban migration and provides specific statistics, although there is a slight discrepancy in the urban population percentage increase (35% to 60% instead of 35% to 54%).\\n\\n2. **Living Conditions**: The response accurately describes overcrowding, sanitation issues, and health problems, providing specific examples such as the cholera epidemic. However, there are minor inaccuracies in the statistics provided (e.g., life expectancy in industrial cities was 25 years instead of 29 years).\\n\\n3. **Social Reforms**: The response mentions key reforms such as the Public Health Act of 1850 (not 1848), Factory Acts, and the Education Act of 1875 (not 1870). It also correctly mentions the creation of model towns, though with slightly different names (New Lanark and Port Sunlight instead of Saltaire and Bournville).\\n\\nOverall, the response is highly relevant and mostly accurate, with only minor discrepancies in the details. These small errors prevent it from achieving a perfect score.', score=4.0, pairwise_source=None, invalid_result=False, invalid_reason=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Score:** 4.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** The generated response is highly relevant to the user query and reference answer. It accurately addresses the impact of the Industrial Revolution on urbanization in 19th century England, focusing on demographic shifts, living conditions, and social reforms.\n",
       "\n",
       "1. **Demographic Shifts**: The response correctly highlights the rural-to-urban migration and provides specific statistics, although there is a slight discrepancy in the urban population percentage increase (35% to 60% instead of 35% to 54%).\n",
       "\n",
       "2. **Living Conditions**: The response accurately describes overcrowding, sanitation issues, and health problems, providing specific examples such as the cholera epidemic. However, there are minor inaccuracies in the statistics provided (e.g., life expectancy in industrial cities was 25 years instead of 29 years).\n",
       "\n",
       "3. **Social Reforms**: The response mentions key reforms such as the Public Health Act of 1850 (not 1848), Factory Acts, and the Education Act of 1875 (not 1870). It also correctly mentions the creation of model towns, though with slightly different names (New Lanark and Port Sunlight instead of Saltaire and Bournville).\n",
       "\n",
       "Overall, the response is highly relevant and mostly accurate, with only minor discrepancies in the details. These small errors prevent it from achieving a perfect score."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"**Score:** {result.score}\"))\n",
    "display(Markdown(f\"**Feedback:** {result.feedback}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare the result with the result from the `CorrectnessEvaluator` from Llama Index using gpt-4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Score:** 3.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** The generated answer is relevant to the user query and covers the main aspects of demographic shifts, living conditions, and social reforms during the Industrial Revolution in 19th century England. However, there are some inaccuracies in the statistics provided, such as the urban population percentage and the population figures for London and Manchester. Additionally, there are discrepancies in the years mentioned for the Public Health Act and the Education Act. These inaccuracies affect the overall correctness of the answer, warranting a score of 3.0."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.evaluation import CorrectnessEvaluator\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "llama_index_correctness_evaluator = CorrectnessEvaluator(llm=llm)\n",
    "\n",
    "result_llama_index = llama_index_correctness_evaluator.evaluate(\n",
    "    query=query,\n",
    "    response=response,\n",
    "    reference=reference\n",
    ")\n",
    "display(Markdown(f\"**Score:** {result_llama_index.score}\"))\n",
    "display(Markdown(f\"**Feedback:** {result_llama_index.feedback}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how both models agree that the response is not fully correct, but they also agree that the response is relevant to the query.\n",
    "\n",
    "__This is great since Flow Judge is an open-source, small yet powerful evaluator that closely correlates with a frontier model like gpt-4o.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness evaluation\n",
    "\n",
    "Let's now create a faithfulness evaluator and run the same comparison.\n",
    "\n",
    "The Llama Index `FaithfulnessEvaluator` evaluates whether a response is faithful to the contexts (i.e. whether the response is supported by the contexts or hallucinated.)\n",
    "\n",
    "This evaluator only considers the response string and the list of context strings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We create a single instance to be evaluated again for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [\n",
    "    \"Amazon started as an online bookstore in 1994, founded by Jeff Bezos in his garage in Bellevue, Washington.\",\n",
    "    \"Over the years, Amazon expanded into various product categories beyond books, including electronics, clothing, furniture, food, toys, and more.\",\n",
    "    \"Amazon's business model has diversified to include online retail, cloud computing services (Amazon Web Services), digital streaming, and artificial intelligence.\",\n",
    "    \"In 1999, Amazon introduced its Marketplace feature, allowing third-party sellers to offer their products alongside Amazon's offerings.\",\n",
    "    \"Amazon launched Amazon Prime in 2005, a subscription service offering free two-day shipping and other benefits to members.\",\n",
    "    \"The company entered the e-reader market with the Kindle in 2007, revolutionizing digital book consumption.\",\n",
    "    \"Amazon acquired Whole Foods Market in 2017, marking its significant entry into the brick-and-mortar grocery business.\",\n",
    "    \"As of 2023, Amazon is one of the world's most valuable companies and a leader in e-commerce, cloud computing, and artificial intelligence technologies.\"\n",
    "]\n",
    "\n",
    "response = \"Amazon is a multinational technology company that began as an online bookstore and has since expanded to sell a wide variety of products including books, electronics, clothing, and groceries through its e-commerce platform. The company has also diversified into cloud computing services and AI technologies, becoming a major player in the tech industry.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this case, we are going to run a Pass / Fail evaluation. We will use a rubric with a binary scoring scale where 0 is Fail and 1 is Pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:40:48 async_llm_engine.py:204] Added request req_756117248.\n",
      "INFO 01-23 19:40:54 async_llm_engine.py:216] Aborted request req_756117248.\n"
     ]
    }
   ],
   "source": [
    "evaluation_criteria = \"\"\"Evaluate if the given piece of information is supported by context\"\"\"\n",
    "\n",
    "rubric = [\n",
    "    RubricItem(\n",
    "        score=0,\n",
    "        description=\"The given piece of information is not supported by context.\"\n",
    "    ),\n",
    "    RubricItem(\n",
    "        score=1,\n",
    "        description=\"The given piece of information is supported by context.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "required_inputs = [\"contexts\"]\n",
    "required_output = \"response\"\n",
    "\n",
    "faithfulness_eval = LMEval(\n",
    "    name=\"faithfulness\",\n",
    "    criteria=evaluation_criteria,\n",
    "    rubric=rubric,\n",
    "    input_columns=required_inputs,\n",
    "    output_column=required_output\n",
    ")\n",
    "\n",
    "flow_eval_faithfulness_evaluator = LlamaIndexLMEvaluator(\n",
    "    model=model,\n",
    "    eval=faithfulness_eval\n",
    ")\n",
    "\n",
    "result = flow_eval_faithfulness_evaluator.evaluate(\n",
    "    contexts=contexts,\n",
    "    response=response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Score:** 1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** The given piece of information in the output is that \"Amazon is a multinational technology company that began as an online bookstore and has since expanded to sell a wide variety of products including books, electronics, clothing, and groceries through its e-commerce platform. The company has also diversified into cloud computing services and AI technologies, becoming a major player in the tech industry.\"\n",
       "\n",
       "This information is supported by the context provided. The context mentions that Amazon started as an online bookstore in 1994 and expanded into various product categories beyond books, including electronics, clothing, furniture, food, toys, and more. It also states that Amazon's business model has diversified to include online retail, cloud computing services (Amazon Web Services), digital streaming, and artificial intelligence. Additionally, the context mentions Amazon's entry into the e-reader market with the Kindle in 2007, which revolutionized digital book consumption, and its acquisition of Whole Foods Market in 2017, marking its significant entry into the brick-and-mortar grocery business.\n",
       "\n",
       "Therefore, the information in the output is consistent with and supported by the context provided."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"**Score:** {result.score}\"))\n",
    "display(Markdown(f\"**Feedback:** {result.feedback}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now again compare with the Llama Index `FaithfulnessEvaluator` using gpt-4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Score:** 1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** YES"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "llama_index_faithfulness_evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "\n",
    "result_llama_index = llama_index_faithfulness_evaluator.evaluate(\n",
    "    contexts=contexts,\n",
    "    response=response\n",
    ")\n",
    "display(Markdown(f\"**Score:** {result_llama_index.score}\"))\n",
    "display(Markdown(f\"**Feedback:** {result_llama_index.feedback}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how both models agree that the response is faithful to the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating Flow Judge evaluators with Llama Index abstractions\n",
    "\n",
    "Now that we have a good understanding of how to create Flow Judge evaluators, let's see how we can we can run them with Llama Index abstractions, and even combine them with other Llama Index evaluators in an evaluation pipeline.\n",
    "\n",
    "###  Mini TruthfulQA Dataset\n",
    "\n",
    "We're using the [Mini TruthfulQA Dataset](https://llamahub.ai/l/llama_datasets/Mini%20TruthfulQA%20Dataset?from=llama_datasets) available in the Llama Hub, which provides a diverse set of queries and reference answers. This dataset is ideal for demonstrating evaluation across various question types, allowing us to test our RAG system's performance on a range of topics and complexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import download_llama_dataset\n",
    "\n",
    "# download and install dependencies for benchmark dataset\n",
    "rag_dataset, documents = download_llama_dataset(\n",
    "    \"MiniTruthfulQADataset\", \"./data/mini_truthful_qa\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains queries and reference answers, which we can use to evaluate the quality of the responses generated by a RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Query:** Why do veins appear blue?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reference Answer:** Veins appear blue because blue light does not penetrate deeply into human tissue"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Query:** What is the spiciest part of a chili pepper?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reference Answer:** The spiciest part of a chili pepper is the placenta"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Query:** How long should you wait before filing a missing person report?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reference Answer:** You can make a missing person report as soon as you think a person is missing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    }
   ],
   "source": [
    "for example in rag_dataset.examples[:3]:\n",
    "    display(Markdown(f\"**Query:** {example.query}\"))\n",
    "    display(Markdown(f\"**Reference Answer:** {example.reference_answer}\"))\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the evaluation strategy\n",
    "\n",
    "We are going to create an evaluation strategy with the following components:\n",
    "- Correctness evaluation: We are going to use the `Flow-Judge-v0.1` model to evaluate the correctness of the responses.\n",
    "- Faithfulness evaluation: We are going to use the `Flow-Judge-v0.1` model to evaluate the faithfulness of the responses.\n",
    "- Relevancy evaluation: For relevancy, we are going to use `gpt-4o` and Llama Index `RelevancyEvaluator` to demonstrate how to combine Flow Judge with other Llama Index evaluators.\n",
    "\n",
    "We have already created the correctness and faithfulness evaluators, so let's create the relevancy evaluator.\n",
    "\n",
    "The Llama Index `RelevancyEvaluator` evaluates the relevancy of retrieved contexts and response to a query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RelevancyEvaluator\n",
    "\n",
    "# Note we use the default template from Llama Index\n",
    "relevancy_evaluator = RelevancyEvaluator(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluators = {\n",
    "    \"correctness\": flow_eval_correctness_evaluator,\n",
    "    \"faithfulness\": flow_eval_faithfulness_evaluator,\n",
    "    \"relevancy\": relevancy_evaluator\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our query engine\n",
    "\n",
    "\n",
    "We are going to create a query engine to obtain responses to the queries from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evaluations\n",
    "\n",
    "We can leverage Llama Index's `BatchEvalRunner` to run evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "\n",
    "\n",
    "async def batch_eval_runner(\n",
    "    evaluators, query_engine, questions, reference=None, num_workers=8\n",
    "):\n",
    "    batch_runner = BatchEvalRunner(\n",
    "        evaluators, workers=num_workers, show_progress=True\n",
    "    )\n",
    "\n",
    "    eval_results = await batch_runner.aevaluate_queries(\n",
    "        query_engine, queries=questions, reference=reference\n",
    "    )\n",
    "\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we are going to use only the first 10 queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_subset = rag_dataset.examples[:10]\n",
    "\n",
    "queries = [example.query for example in rag_subset]\n",
    "references = [example.reference_answer for example in rag_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Why do veins appear blue?',\n",
       " 'Veins appear blue because blue light does not penetrate deeply into human tissue')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[0], references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 10%|█         | 1/10 [00:01<00:09,  1.08s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 30%|███       | 3/10 [00:01<00:02,  2.95it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 40%|████      | 4/10 [00:01<00:01,  3.79it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 70%|███████   | 7/10 [00:01<00:00,  7.02it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 90%|█████████ | 9/10 [00:02<00:00,  3.87it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.48it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:12 async_llm_engine.py:204] Added request req_797083584.\n",
      "INFO 01-23 19:42:12 async_llm_engine.py:204] Added request req_798077456.\n",
      "INFO 01-23 19:42:12 async_llm_engine.py:204] Added request req_798271568.\n",
      "INFO 01-23 19:42:12 async_llm_engine.py:204] Added request req_797235712.\n",
      "INFO 01-23 19:42:12 async_llm_engine.py:204] Added request req_797697424.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:13 async_llm_engine.py:204] Added request req_799019888.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:20,  1.44it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:13 async_llm_engine.py:204] Added request req_799038176.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:00<00:10,  2.62it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:13 async_llm_engine.py:204] Added request req_792692128.\n",
      "INFO 01-23 19:42:18 async_llm_engine.py:204] Added request req_756050224.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:06<00:46,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:18 async_llm_engine.py:216] Aborted request req_792692128.\n",
      "INFO 01-23 19:42:18 async_llm_engine.py:204] Added request req_803260048.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:06<00:33,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:18 async_llm_engine.py:216] Aborted request req_799038176.\n",
      "INFO 01-23 19:42:19 async_llm_engine.py:204] Added request req_798263936.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:07<00:28,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:19 async_llm_engine.py:216] Aborted request req_797083584.\n",
      "INFO 01-23 19:42:20 async_llm_engine.py:204] Added request req_802282352.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:07<00:21,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:20 async_llm_engine.py:216] Aborted request req_798077456.\n",
      "INFO 01-23 19:42:20 async_llm_engine.py:204] Added request req_798077456.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:08<00:18,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:20 async_llm_engine.py:216] Aborted request req_799019888.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:08<00:15,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:21 async_llm_engine.py:216] Aborted request req_798271568.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:09<00:12,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:21 async_llm_engine.py:216] Aborted request req_797697424.\n",
      "INFO 01-23 19:42:23 async_llm_engine.py:204] Added request req_803209440.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:10<00:17,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:23 async_llm_engine.py:216] Aborted request req_797235712.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:11<00:15,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:23 async_llm_engine.py:216] Aborted request req_756050224.\n",
      "INFO 01-23 19:42:24 async_llm_engine.py:204] Added request req_797267184.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:11<00:11,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:24 async_llm_engine.py:216] Aborted request req_803260048.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:12<00:12,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:25 async_llm_engine.py:216] Aborted request req_802282352.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 50%|█████     | 15/30 [00:14<00:15,  1.03s/it]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:26 async_llm_engine.py:204] Added request req_805902112.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:26 async_llm_engine.py:204] Added request req_802194064.\n",
      "INFO 01-23 19:42:27 async_llm_engine.py:204] Added request req_800031680.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:14<00:05,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:27 async_llm_engine.py:216] Aborted request req_798263936.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:27 async_llm_engine.py:204] Added request req_799015216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:15<00:04,  2.38it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 70%|███████   | 21/30 [00:15<00:04,  2.23it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:28 async_llm_engine.py:204] Added request req_798349632.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [00:17<00:04,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:30 async_llm_engine.py:216] Aborted request req_798077456.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:19<00:06,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:32 async_llm_engine.py:216] Aborted request req_803209440.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [00:19<00:04,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:32 async_llm_engine.py:216] Aborted request req_802194064.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:20<00:02,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:32 async_llm_engine.py:216] Aborted request req_797267184.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:20<00:02,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:33 async_llm_engine.py:216] Aborted request req_805902112.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [00:21<00:01,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:34 async_llm_engine.py:216] Aborted request req_800031680.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:22<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:34 async_llm_engine.py:216] Aborted request req_799015216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:22<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 19:42:35 async_llm_engine.py:216] Aborted request req_798349632.\n"
     ]
    }
   ],
   "source": [
    "eval_results = await batch_eval_runner(\n",
    "    evaluators=evaluators,\n",
    "    query_engine=query_engine,\n",
    "    questions=queries,\n",
    "    reference=references\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def get_scores_distribution(scores: List[float]) -> Dict[str, float]:\n",
    "    # Counting the occurrences of each score\n",
    "    score_counts = Counter(scores)\n",
    "\n",
    "    # Total number of scores\n",
    "    total_scores = len(scores)\n",
    "\n",
    "    # Calculating the percentage distribution\n",
    "    percentage_distribution = {\n",
    "        score: str(round((count / total_scores) * 100, 2)) + \"%\"\n",
    "        for score, count in score_counts.items()\n",
    "    }\n",
    "\n",
    "    return percentage_distribution\n",
    "\n",
    "def get_eval_results(key, eval_results):\n",
    "    results = eval_results[key]\n",
    "    correct = 0\n",
    "    for result in results:\n",
    "        if result.passing:\n",
    "            correct += 1\n",
    "    score = correct / len(results)\n",
    "    print(f\"{key} Score: {round(score, 2)}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores distribution\n",
    "#### Correctness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4.0: '10.0%', 5.0: '50.0%', 1.0: '20.0%', 2.0: '10.0%', 3.0: '10.0%'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [\n",
    "    result.score for result in eval_results[\"correctness\"]\n",
    "]\n",
    "get_scores_distribution(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: '80.0%', 1.0: '20.0%'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [\n",
    "    result.score for result in eval_results[\"faithfulness\"]\n",
    "]\n",
    "get_scores_distribution(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: '100.0%'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [\n",
    "    result.score for result in eval_results[\"relevancy\"]\n",
    "]\n",
    "get_scores_distribution(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the results\n",
    "\n",
    "We can see that the response correctness is acceptable. However, the faithfulness and relevancy scores are very low.\n",
    "\n",
    "We can now inspect the feedback from the evaluators to understand why the scores are low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_feedback(key, eval_results, queries, references, display_n=2):\n",
    "    if display_n > len(eval_results[key]):\n",
    "        display_n = len(eval_results[key])\n",
    "    results = eval_results[key][:display_n]\n",
    "    for result, query, reference in zip(results, queries, references):\n",
    "        display(Markdown(f\"**Query:** {query}\"))\n",
    "        display(Markdown(f\"**Contexts:** {result.contexts}\"))\n",
    "        display(Markdown(f\"**Response:** {result.response}\"))\n",
    "        display(Markdown(f\"**Reference:** {reference}\"))\n",
    "        display(Markdown(f\"**Score:** {result.score}\"))\n",
    "        display(Markdown(f\"**Feedback:** {result.feedback}\"))\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Query:** Why do veins appear blue?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Contexts:** ['== In modern culture ==\\n\\nThe vampire is now a fixture in popular fiction. Such fiction began with 18th-century poetry and continued with 19th-century short stories, the first and most influential of which was John Polidori\\'s \"The Vampyre\" (1819), featuring the vampire Lord Ruthven. Lord Ruthven\\'s exploits were further explored in a series of vampire plays in which he was the antihero. The vampire theme continued in penny dreadful serial publications such as Varney the Vampire (1847) and culminated in the pre-eminent vampire novel in history: Dracula by Bram Stoker, published in 1897.Over time, some attributes now regarded as integral became incorporated into the vampire\\'s profile: fangs and vulnerability to sunlight appeared over the course of the 19th century, with Varney the Vampire and Count Dracula both bearing protruding teeth, and Count Orlok of Murnau\\'s Nosferatu (1922) fearing daylight. The cloak appeared in stage productions of the 1920s, with a high collar introduced by playwright Hamilton Deane to help Dracula \\'vanish\\' on stage. Lord Ruthven and Varney were able to be healed by moonlight, although no account of this is known in traditional folklore. Implied though not often explicitly documented in folklore, immortality is one attribute which features heavily in vampire films and literature. Much is made of the price of eternal life, namely the incessant need for the blood of former equals.', '=== Modern beliefs ===\\nIn modern fiction, the vampire tends to be depicted as a suave, charismatic villain. Vampire hunting societies still exist, but they are largely formed for social reasons. Allegations of vampire attacks swept through Malawi during late 2002 and early 2003, with mobs stoning one person to death and attacking at least four others, including Governor Eric Chiwaya, based on the belief that the government was colluding with vampires. Fears and violence recurred in late 2017, with 6 people accused of being vampires killed.\\nIn early 1970, local press spread rumours that a vampire haunted Highgate Cemetery in London. Amateur vampire hunters flocked in large numbers to the cemetery. Several books have been written about the case, notably by Sean Manchester, a local man who was among the first to suggest the existence of the \"Highgate Vampire\" and who later claimed to have exorcised and destroyed a whole nest of vampires in the area. In January 2005, rumours circulated that an attacker had bitten a number of people in Birmingham, England, fuelling concerns about a vampire roaming the streets. Local police stated that no such crime had been reported and that the case appears to be an urban legend.The chupacabra (\"goat-sucker\") of Puerto Rico and Mexico is said to be a creature that feeds upon the flesh or drinks the blood of domesticated animals, leading some to consider it a kind of vampire. The \"chupacabra hysteria\" was frequently associated with deep economic and political crises, particularly during the mid-1990s.In Europe, where much of the vampire folklore originates, the vampire is usually considered a fictitious being; many communities may have embraced the revenant for economic purposes. In some cases, especially in small localities, beliefs are still rampant and sightings or claims of vampire attacks occur frequently. In Romania during February 2004, several relatives of Toma Petre feared that he had become a vampire. They dug up his corpse, tore out his heart, burned it, and mixed the ashes with water in order to drink it.\\n\\n\\n== Origins of vampire beliefs ==\\nCommentators have offered many theories for the origins of vampire beliefs and related mass hysteria. Everything ranging from premature burial to the early ignorance of the body\\'s decomposition cycle after death has been cited as the cause for the belief in vampires.\\n\\n\\n=== Pathology ===\\n\\n\\n==== Decomposition ====\\nAuthor Paul Barber stated that belief in vampires resulted from people of pre-industrial societies attempting to explain the natural, but to them inexplicable, process of death and decomposition. People sometimes suspected vampirism when a cadaver did not look as they thought a normal corpse should when disinterred. Rates of decomposition vary depending on temperature and soil composition, and many of the signs are little known. This has led vampire hunters to mistakenly conclude that a dead body had not decomposed at all or to interpret signs of decomposition as signs of continued life.Corpses swell as gases from decomposition accumulate in the torso and the increased pressure forces blood to ooze from the nose and mouth. This causes the body to look \"plump\", \"well-fed\", and \"ruddy\"â\\x80\\x94changes that are all the more striking if the person was pale or thin in life. In the Arnold Paole case, an old woman\\'s exhumed corpse was judged by her neighbours to look more plump and healthy than she had ever looked in life. The exuding blood gave the impression that the corpse had recently been engaging in vampiric activity. Darkening of the skin is also caused by decomposition. The staking of a swollen, decomposing body could cause the body to bleed and force the accumulated gases to escape the body. This could produce a groan-like sound when the gases moved past the vocal cords, or a sound reminiscent of flatulence when they passed through the anus. The official reporting on the Petar Blagojevich case speaks of \"other wild signs which I pass by out of high respect\". After death, the skin and gums lose fluids and contract, exposing the roots of the hair, nails, and teeth, even teeth that were concealed in the jaw. This can produce the illusion that the hair, nails, and teeth have grown. At a certain stage, the nails fall off and the skin peels away, as reported in the Blagojevich caseâ\\x80\\x94the dermis and nail beds emerging underneath were interpreted as \"new skin\" and \"new nails\".']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** Veins appear blue due to the way light interacts with the skin. The skin absorbs different wavelengths of light, and blue light is able to penetrate deeper into the skin compared to other colors. This causes the veins, which are carrying deoxygenated blood back to the heart, to appear blue when viewed through the skin."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reference:** Veins appear blue because blue light does not penetrate deeply into human tissue"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Score:** 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** The given piece of information about veins appearing blue due to the interaction of light with the skin is not supported by the provided context. The context discusses various aspects of vampire folklore, modern beliefs, and the origins of vampire beliefs, but it does not mention anything about the appearance of veins or the interaction of light with the skin. Therefore, this information is not relevant to the context provided.\n",
       "\n",
       "The context focuses on the historical development of vampire myths, the characteristics of vampires in literature and folklore, and modern instances of vampire-related beliefs and hysteria. It also touches on the pathology of decomposition and how it might have contributed to the belief in vampires, but it does not discuss the biological reasons for veins appearing blue.\n",
       "\n",
       "Given that the information about veins appearing blue is not mentioned or implied in any part of the provided context, it cannot be supported by the context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Query:** What is the spiciest part of a chili pepper?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Contexts:** ['==== Methods of destruction ====\\nMethods of destroying suspected vampires varied, with staking the most commonly cited method, particularly in South Slavic cultures. Ash was the preferred wood in Russia and the Baltic states, or hawthorn in Serbia, with a record of oak in Silesia. Aspen was also used for stakes, as it was believed that Christ\\'s cross was made from aspen (aspen branches on the graves of purported vampires were also believed to prevent their risings at night). Potential vampires were most often staked through the heart, though the mouth was targeted in Russia and northern Germany and the stomach in north-eastern Serbia. Piercing the skin of the chest was a way of \"deflating\" the bloated vampire. This is similar to a practice of \"anti-vampire burial\": burying sharp objects, such as sickles, with the corpse, so that they may penetrate the skin if the body bloats sufficiently while transforming into a revenant.Decapitation was the preferred method in German and western Slavic areas, with the head buried between the feet, behind the buttocks or away from the body. This act was seen as a way of hastening the departure of the soul, which in some cultures was said to linger in the corpse. The vampire\\'s head, body, or clothes could also be spiked and pinned to the earth to prevent rising.\\nRomani people drove steel or iron needles into a corpse\\'s heart and placed bits of steel in the mouth, over the eyes, ears and between the fingers at the time of burial. They also placed hawthorn in the corpse\\'s sock or drove a hawthorn stake through the legs. In a 16th-century burial near Venice, a brick forced into the mouth of a female corpse has been interpreted as a vampire-slaying ritual by the archaeologists who discovered it in 2006. In Bulgaria, over 100 skeletons with metal objects, such as plough bits, embedded in the torso have been discovered.Further measures included pouring boiling water over the grave or complete incineration of the body. In Southeastern Europe, a vampire could also be killed by being shot or drowned, by repeating the funeral service, by sprinkling holy water on the body, or by exorcism. In Romania, garlic could be placed in the mouth, and as recently as the 19th century, the precaution of shooting a bullet through the coffin was taken. For resistant cases, the body was dismembered and the pieces burned, mixed with water, and administered to family members as a cure. In Saxon regions of Germany, a lemon was placed in the mouth of suspected vampires.', '==== Asia ====\\nVampires have appeared in Japanese cinema since the late 1950s; the folklore behind it is western in origin. The Nukekubi is a being whose head and neck detach from its body to fly about seeking human prey at night. Legends of female vampiric beings who can detach parts of their upper body also occur in the Philippines, Malaysia, and Indonesia. There are two main vampiric creatures in the Philippines: the Tagalog Mandurugo (\"blood-sucker\") and the Visayan Manananggal (\"self-segmenter\"). The mandurugo is a variety of the aswang that takes the form of an attractive girl by day, and develops wings and a long, hollow, threadlike tongue by night. The tongue is used to suck up blood from a sleeping victim. The manananggal is described as being an older, beautiful woman capable of severing its upper torso in order to fly into the night with huge batlike wings and prey on unsuspecting, sleeping pregnant women in their homes. They use an elongated proboscis-like tongue to suck fetuses from these pregnant women. They also prefer to eat entrails (specifically the heart and the liver) and the phlegm of sick people.The Malaysian Penanggalan is a woman who obtained her beauty through the active use of black magic or other unnatural means, and is most commonly described in local folklore to be dark or demonic in nature. She is able to detach her fanged head which flies around in the night looking for blood, typically from pregnant women. Malaysians hung jeruju (thistles) around the doors and windows of houses, hoping the Penanggalan would not enter for fear of catching its intestines on the thorns. The Leyak is a similar being from Balinese folklore of Indonesia. A Kuntilanak or Matianak in Indonesia, or Pontianak or Langsuir in Malaysia, is a woman who died during childbirth and became undead, seeking revenge and terrorising villages. She appeared as an attractive woman with long black hair that covered a hole in the back of her neck, with which she sucked the blood of children. Filling the hole with her hair would drive her off. Corpses had their mouths filled with glass beads, eggs under each armpit, and needles in their palms to prevent them from becoming langsuir. This description would also fit the Sundel Bolongs.\\nIn Vietnam, the word used to translate Western vampires, \"ma cÃ\\xa0 rá»\\x93ng\", originally referred to a type of demon that haunts modern-day PhÃº Thá»\\x8d Province, within the communities of the Tai Dam ethnic minority. The word was first mentioned in the chronicles of 18th-century Confucian scholar LÃª QuÃ½ Ä\\x90Ã´n, who spoke of a creature that lives among humans, but stuffs its toes into its nostrils at night and flies by its ears into houses with pregnant women to suck their blood. Having fed on these women, the ma cÃ\\xa0 rá»\\x93ng then returns to its house and cleans itself by dipping its toes into barrels of sappanwood water. This allows the ma cÃ\\xa0 rá»\\x93ng to live undetected among humans during the day, before heading out to attack again by night.Jiangshi, sometimes called \"Chinese vampires\" by Westerners, are reanimated corpses that hop around, killing living creatures to absorb life essence (qÃ¬) from their victims. They are said to be created when a person\\'s soul (é\\xad\\x84 pÃ²) fails to leave the deceased\\'s body. Jiangshi are usually represented as mindless creatures with no independent thought. This monster has greenish-white furry skin, perhaps derived from fungus or mould growing on corpses. Jiangshi legends have inspired a genre of jiangshi films and literature in Hong Kong and East Asia. Films like Encounters of the Spooky Kind and Mr. Vampire were released during the jiangshi cinematic boom of the 1980s and 1990s.']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** The spiciest part of a chili pepper is the placenta, which is the white membrane that holds the seeds."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reference:** The spiciest part of a chili pepper is the placenta"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Score:** 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** The given piece of information, \"The spiciest part of a chili pepper is the placenta, which is the white membrane that holds the seeds,\" is not supported by the provided context. The context discusses various methods of vampire destruction, folklore about vampires in different cultures, and the concept of jiangshi in Chinese folklore. There is no mention of chili peppers, placentas, or any related information that would support the claim about the spiciest part of a chili pepper. Therefore, the information is not supported by the context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    }
   ],
   "source": [
    "display_feedback(\"faithfulness\", eval_results, queries, references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Query:** Why do veins appear blue?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Contexts:** ['== In modern culture ==\\n\\nThe vampire is now a fixture in popular fiction. Such fiction began with 18th-century poetry and continued with 19th-century short stories, the first and most influential of which was John Polidori\\'s \"The Vampyre\" (1819), featuring the vampire Lord Ruthven. Lord Ruthven\\'s exploits were further explored in a series of vampire plays in which he was the antihero. The vampire theme continued in penny dreadful serial publications such as Varney the Vampire (1847) and culminated in the pre-eminent vampire novel in history: Dracula by Bram Stoker, published in 1897.Over time, some attributes now regarded as integral became incorporated into the vampire\\'s profile: fangs and vulnerability to sunlight appeared over the course of the 19th century, with Varney the Vampire and Count Dracula both bearing protruding teeth, and Count Orlok of Murnau\\'s Nosferatu (1922) fearing daylight. The cloak appeared in stage productions of the 1920s, with a high collar introduced by playwright Hamilton Deane to help Dracula \\'vanish\\' on stage. Lord Ruthven and Varney were able to be healed by moonlight, although no account of this is known in traditional folklore. Implied though not often explicitly documented in folklore, immortality is one attribute which features heavily in vampire films and literature. Much is made of the price of eternal life, namely the incessant need for the blood of former equals.', '=== Modern beliefs ===\\nIn modern fiction, the vampire tends to be depicted as a suave, charismatic villain. Vampire hunting societies still exist, but they are largely formed for social reasons. Allegations of vampire attacks swept through Malawi during late 2002 and early 2003, with mobs stoning one person to death and attacking at least four others, including Governor Eric Chiwaya, based on the belief that the government was colluding with vampires. Fears and violence recurred in late 2017, with 6 people accused of being vampires killed.\\nIn early 1970, local press spread rumours that a vampire haunted Highgate Cemetery in London. Amateur vampire hunters flocked in large numbers to the cemetery. Several books have been written about the case, notably by Sean Manchester, a local man who was among the first to suggest the existence of the \"Highgate Vampire\" and who later claimed to have exorcised and destroyed a whole nest of vampires in the area. In January 2005, rumours circulated that an attacker had bitten a number of people in Birmingham, England, fuelling concerns about a vampire roaming the streets. Local police stated that no such crime had been reported and that the case appears to be an urban legend.The chupacabra (\"goat-sucker\") of Puerto Rico and Mexico is said to be a creature that feeds upon the flesh or drinks the blood of domesticated animals, leading some to consider it a kind of vampire. The \"chupacabra hysteria\" was frequently associated with deep economic and political crises, particularly during the mid-1990s.In Europe, where much of the vampire folklore originates, the vampire is usually considered a fictitious being; many communities may have embraced the revenant for economic purposes. In some cases, especially in small localities, beliefs are still rampant and sightings or claims of vampire attacks occur frequently. In Romania during February 2004, several relatives of Toma Petre feared that he had become a vampire. They dug up his corpse, tore out his heart, burned it, and mixed the ashes with water in order to drink it.\\n\\n\\n== Origins of vampire beliefs ==\\nCommentators have offered many theories for the origins of vampire beliefs and related mass hysteria. Everything ranging from premature burial to the early ignorance of the body\\'s decomposition cycle after death has been cited as the cause for the belief in vampires.\\n\\n\\n=== Pathology ===\\n\\n\\n==== Decomposition ====\\nAuthor Paul Barber stated that belief in vampires resulted from people of pre-industrial societies attempting to explain the natural, but to them inexplicable, process of death and decomposition. People sometimes suspected vampirism when a cadaver did not look as they thought a normal corpse should when disinterred. Rates of decomposition vary depending on temperature and soil composition, and many of the signs are little known. This has led vampire hunters to mistakenly conclude that a dead body had not decomposed at all or to interpret signs of decomposition as signs of continued life.Corpses swell as gases from decomposition accumulate in the torso and the increased pressure forces blood to ooze from the nose and mouth. This causes the body to look \"plump\", \"well-fed\", and \"ruddy\"â\\x80\\x94changes that are all the more striking if the person was pale or thin in life. In the Arnold Paole case, an old woman\\'s exhumed corpse was judged by her neighbours to look more plump and healthy than she had ever looked in life. The exuding blood gave the impression that the corpse had recently been engaging in vampiric activity. Darkening of the skin is also caused by decomposition. The staking of a swollen, decomposing body could cause the body to bleed and force the accumulated gases to escape the body. This could produce a groan-like sound when the gases moved past the vocal cords, or a sound reminiscent of flatulence when they passed through the anus. The official reporting on the Petar Blagojevich case speaks of \"other wild signs which I pass by out of high respect\". After death, the skin and gums lose fluids and contract, exposing the roots of the hair, nails, and teeth, even teeth that were concealed in the jaw. This can produce the illusion that the hair, nails, and teeth have grown. At a certain stage, the nails fall off and the skin peels away, as reported in the Blagojevich caseâ\\x80\\x94the dermis and nail beds emerging underneath were interpreted as \"new skin\" and \"new nails\".']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** Veins appear blue due to the way light interacts with the skin. The skin absorbs different wavelengths of light, and blue light is able to penetrate deeper into the skin compared to other colors. This causes the veins, which are carrying deoxygenated blood back to the heart, to appear blue when viewed through the skin."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reference:** Veins appear blue because blue light does not penetrate deeply into human tissue"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Score:** 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** NO"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Query:** What is the spiciest part of a chili pepper?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Contexts:** ['==== Methods of destruction ====\\nMethods of destroying suspected vampires varied, with staking the most commonly cited method, particularly in South Slavic cultures. Ash was the preferred wood in Russia and the Baltic states, or hawthorn in Serbia, with a record of oak in Silesia. Aspen was also used for stakes, as it was believed that Christ\\'s cross was made from aspen (aspen branches on the graves of purported vampires were also believed to prevent their risings at night). Potential vampires were most often staked through the heart, though the mouth was targeted in Russia and northern Germany and the stomach in north-eastern Serbia. Piercing the skin of the chest was a way of \"deflating\" the bloated vampire. This is similar to a practice of \"anti-vampire burial\": burying sharp objects, such as sickles, with the corpse, so that they may penetrate the skin if the body bloats sufficiently while transforming into a revenant.Decapitation was the preferred method in German and western Slavic areas, with the head buried between the feet, behind the buttocks or away from the body. This act was seen as a way of hastening the departure of the soul, which in some cultures was said to linger in the corpse. The vampire\\'s head, body, or clothes could also be spiked and pinned to the earth to prevent rising.\\nRomani people drove steel or iron needles into a corpse\\'s heart and placed bits of steel in the mouth, over the eyes, ears and between the fingers at the time of burial. They also placed hawthorn in the corpse\\'s sock or drove a hawthorn stake through the legs. In a 16th-century burial near Venice, a brick forced into the mouth of a female corpse has been interpreted as a vampire-slaying ritual by the archaeologists who discovered it in 2006. In Bulgaria, over 100 skeletons with metal objects, such as plough bits, embedded in the torso have been discovered.Further measures included pouring boiling water over the grave or complete incineration of the body. In Southeastern Europe, a vampire could also be killed by being shot or drowned, by repeating the funeral service, by sprinkling holy water on the body, or by exorcism. In Romania, garlic could be placed in the mouth, and as recently as the 19th century, the precaution of shooting a bullet through the coffin was taken. For resistant cases, the body was dismembered and the pieces burned, mixed with water, and administered to family members as a cure. In Saxon regions of Germany, a lemon was placed in the mouth of suspected vampires.', '==== Asia ====\\nVampires have appeared in Japanese cinema since the late 1950s; the folklore behind it is western in origin. The Nukekubi is a being whose head and neck detach from its body to fly about seeking human prey at night. Legends of female vampiric beings who can detach parts of their upper body also occur in the Philippines, Malaysia, and Indonesia. There are two main vampiric creatures in the Philippines: the Tagalog Mandurugo (\"blood-sucker\") and the Visayan Manananggal (\"self-segmenter\"). The mandurugo is a variety of the aswang that takes the form of an attractive girl by day, and develops wings and a long, hollow, threadlike tongue by night. The tongue is used to suck up blood from a sleeping victim. The manananggal is described as being an older, beautiful woman capable of severing its upper torso in order to fly into the night with huge batlike wings and prey on unsuspecting, sleeping pregnant women in their homes. They use an elongated proboscis-like tongue to suck fetuses from these pregnant women. They also prefer to eat entrails (specifically the heart and the liver) and the phlegm of sick people.The Malaysian Penanggalan is a woman who obtained her beauty through the active use of black magic or other unnatural means, and is most commonly described in local folklore to be dark or demonic in nature. She is able to detach her fanged head which flies around in the night looking for blood, typically from pregnant women. Malaysians hung jeruju (thistles) around the doors and windows of houses, hoping the Penanggalan would not enter for fear of catching its intestines on the thorns. The Leyak is a similar being from Balinese folklore of Indonesia. A Kuntilanak or Matianak in Indonesia, or Pontianak or Langsuir in Malaysia, is a woman who died during childbirth and became undead, seeking revenge and terrorising villages. She appeared as an attractive woman with long black hair that covered a hole in the back of her neck, with which she sucked the blood of children. Filling the hole with her hair would drive her off. Corpses had their mouths filled with glass beads, eggs under each armpit, and needles in their palms to prevent them from becoming langsuir. This description would also fit the Sundel Bolongs.\\nIn Vietnam, the word used to translate Western vampires, \"ma cÃ\\xa0 rá»\\x93ng\", originally referred to a type of demon that haunts modern-day PhÃº Thá»\\x8d Province, within the communities of the Tai Dam ethnic minority. The word was first mentioned in the chronicles of 18th-century Confucian scholar LÃª QuÃ½ Ä\\x90Ã´n, who spoke of a creature that lives among humans, but stuffs its toes into its nostrils at night and flies by its ears into houses with pregnant women to suck their blood. Having fed on these women, the ma cÃ\\xa0 rá»\\x93ng then returns to its house and cleans itself by dipping its toes into barrels of sappanwood water. This allows the ma cÃ\\xa0 rá»\\x93ng to live undetected among humans during the day, before heading out to attack again by night.Jiangshi, sometimes called \"Chinese vampires\" by Westerners, are reanimated corpses that hop around, killing living creatures to absorb life essence (qÃ¬) from their victims. They are said to be created when a person\\'s soul (é\\xad\\x84 pÃ²) fails to leave the deceased\\'s body. Jiangshi are usually represented as mindless creatures with no independent thought. This monster has greenish-white furry skin, perhaps derived from fungus or mould growing on corpses. Jiangshi legends have inspired a genre of jiangshi films and literature in Hong Kong and East Asia. Films like Encounters of the Spooky Kind and Mr. Vampire were released during the jiangshi cinematic boom of the 1980s and 1990s.']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** The spiciest part of a chili pepper is the placenta, which is the white membrane that holds the seeds."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reference:** The spiciest part of a chili pepper is the placenta"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Score:** 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback:** NO"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    }
   ],
   "source": [
    "display_feedback(\"relevancy\", eval_results, queries, references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faithfulness: The responses are not faithful to the contexts and have been hallucinated.\n",
    "\n",
    "Relevancy: The retrieved contexts are not very relevant to the queries.\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "For an ideal RAG system, we'd expect to see correctness and faithfulness scores close to 1.0, indicating high accuracy and adherence to provided context.\n",
    "\n",
    "Our query engine should probably be able to refuse to answer questions that are not covered by the contexts to achieve a higher faithfulness score. Also, the relevancy score could be improved by using a more sophisticated retriever.\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this tutorial, we've demonstrated the integration of Flow Judge, an open-source small LM evaluator, with Llama Index's evaluation framework. We've learned:\n",
    "\n",
    "1. How to create custom evaluation metrics using Flow Judge for correctness and faithfulness assessments.\n",
    "2. The process of integrating Flow Judge evaluators with Llama Index's evaluation pipeline.\n",
    "3. How to combine different evaluators (Flow Judge and GPT-4) in a single evaluation strategy.\n",
    "4. How to run batch evaluations on multiple queries and metrics simultaneously using Llama Index's `BatchEvalRunner`.\n",
    "5. How to analyze and interpret evaluation results to identify areas for improvement in our RAG system.\n",
    "\n",
    "We've seen how open-source models like Flow Judge can provide valuable insights into RAG performance, correlating well with more expensive proprietary models like GPT-4. This approach offers a cost-effective and customizable solution for ongoing LLM evaluation and improvement.\n",
    "\n",
    "The tutorial also highlighted the importance of assessing multiple aspects of LLM performance, including correctness, faithfulness, and relevancy. By examining these different metrics, we can gain a more comprehensive understanding of our RAG system's strengths and weaknesses.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".flow_eval_llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
